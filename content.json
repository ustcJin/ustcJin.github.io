{"meta":{"title":"璀璨星河，伴我同行","subtitle":null,"description":null,"author":"ustcJin","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2017-02-14T07:05:18.000Z","updated":"2017-02-14T07:05:18.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-02-14T07:03:14.000Z","updated":"2017-02-14T07:03:14.000Z","comments":true,"path":"tags/index-1.html","permalink":"http://yoursite.com/tags/index-1.html","excerpt":"","text":""},{"title":"tags","date":"2017-02-14T07:01:33.000Z","updated":"2017-02-14T07:20:22.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":"title: 标签测试文章tags: test it"}],"posts":[{"title":"svm","slug":"svm","date":"2020-07-06T14:28:04.000Z","updated":"2020-07-20T15:06:47.000Z","comments":true,"path":"2020/07/06/svm/","link":"","permalink":"http://yoursite.com/2020/07/06/svm/","excerpt":"","text":"几个问题 什么是超平面 法向量？向量相乘? 点到超平面距离? 如何把$y_i*(w*x_i+b)$强制置为1? 超平面就是原来的空间减少一个维度的平面, 超平面用$w^T*x+b=0$表示 点到超平面的距离点$x_0$投影到超平面上的点为$x_1$，可知$w*x_1+b=0$,法向量和垂直向量是平行关系，论述中$x_0, x_1, w$均为N维向量, d为所求的距离易知，两点的连线$\\vec{x_0x_1}$垂直于平面，即和法向量平行，单位法向量可表示为$$\\vec{I} = \\frac{w}{|w|}$$ 即有$$\\vec{x_0x_1}=\\vec{I}*d$$$$=\\frac{w}{|w|}*d$$，故有$$|w*\\vec{x_0x_1}|=|w*\\frac{w}{|w|}*d|=|\\frac{w_1^2+w_2^2+…+w_n^2}{|w|}*d|=|w|d$$$$w*\\vec{x_0x_1}=w^1(x_0^1-x_1^1)+w^2(x_0^2-x_1^2)+…+w^N(x_0^N-x_1^N)$$$$=w^1x_0^1+w^2x_0^2+…+w^Nx_0^N-(w^1x_1^1+w^2x_1^2+…+w^Nx_1^N)$$$$=w*x_0+b$$$$|w|d=|w*x_0+b|$$ 得出 $$d=\\frac{|w*x_0+b|}{|w|}$$这个求得就是集合距离，而非函数距离，函数距离是分子，调节$w,b$可以无限大，没有意义，所以要加w的L2正则项作为分母。 求最优解只分析support vector上的点，负例取-1, 正例取+1，所以 $d=y*\\frac{w*x_0+b}{|w|}$，如何$\\max{d}$?，超平面通过调整$w$和$b$可将support vector上的点的函数值变为1和-1，这样问题变为求$$\\min{\\frac{1}{|w|}}$$ 且满足条件，对所有的样本$x$满足$$y*(w*x+b)&gt;=1$$ 为何要设置support vec上的点的$y*(w*x+b)=1$?目标是$$Obj=\\max{\\frac{1}{|w|}}\\min{y_i*(w*x_i+b)}, i \\in {1,2,…,n}$$可以做到让$\\min{y_i*(w*x_i+b)}, i \\in {1,2,…,n} = 1$ ？可见对w做限制皆可，即上面的限制条件，如果控制|w|不变，则原来的目标仍然不能简化(max, min)，且有两个变量，而如果按照上面的设置(即最小的函数距离设置为1)，则完全可以做到。 拉格朗日乘子？KKT条件？上述条件可整理如下$$Obj=\\min{\\frac{1}{2}*|w|^2}$$$$s.t. w*x_i+b=1;i=0,1,…,n$$引入拉个朗日乘子$\\alpha$$$Objx=\\frac{1}{2}*|w|^2-\\sum^n_{i=1}{\\alpha*(w*x_i+b-1)}; m=1,2,…,m$$由于$w*x_i+b&gt;=1$, 所以$$\\max{Objx} = \\frac{1}{2}*|w|^2$$ 即$\\alpha$满足引入拉格朗日乘子是将目标变成无约束条件 $$\\alpha =\\begin{cases}= 0 &amp; w*x_i+b&gt;1 \\newline\\gt 0 &amp; w*x_i+b=1\\end{cases}$$不在support vector上点的$\\alpha$都等于0，支持向量上的$\\alpha$大于0, 于是可得$$Obj=\\min{\\max{Objx}}$$可以转成对偶问题，即最小的最大值能否转化为最大的最小值？答案是强对偶条件下是等价的，而一般的凸问题优化都是满足强对偶条件的。然后用KKT条件求解，极值点在w和b求导=0处，联立可以求$\\alpha$的范围","categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"}]},{"title":"EM算法","slug":"EM","date":"2019-11-04T12:50:14.000Z","updated":"2019-12-25T15:09:38.000Z","comments":true,"path":"2019/11/04/EM/","link":"","permalink":"http://yoursite.com/2019/11/04/EM/","excerpt":"","text":"EM算法和最大似然的区别MLE = Max Likelihood Estimation，最大似然估计，这个可以解一般的问题，比如投硬币10次，9次朝上，1次朝下，问硬币朝上的概率是多少？则$$MLE = arg \\max_{p} p^{9}(1-p)$$如何求最大值，对参数求导=0即可，这个不好算，一般最大似然会取Log，这样求导会方便一些$$log(p^9(1-p)) = 9logp + log(1-p)$$很容易得到p=0.9时，最大似然成立。有时候的问题这么直接求解解不了，还有一个隐含变量，比如男女测身高估计正态分布的参数，或者两个隐蔽测试估计每个硬币的上下的概率，可以看到升高的case里，需要知道是男是女，硬币的case里需要知道选取的哪个硬币，这些隐变量有了才能列出似然函数，然而多一个参数显然就不好解了，这个时候就需要用到EM算法，通过迭代的方法，不断逼近真实的分布。 原理逼近$$\\theta = \\max_{\\theta} L(\\theta) = \\max_{\\theta} log(P(x^{i}|\\theta))$$增加了隐变量后$\\theta$后，似然函数变为$$\\theta, z = \\max_{\\theta,z} L(\\theta, z) = \\max_{\\theta,z} \\sum_{i=1}^{m} log \\sum_{z^{(i)}}P(x^{i},z^{i}|\\theta)$$引入一个未知的新的分布$Q_i(z^i)$, 即关于隐变量z的分布，则上面的目标变为$$\\sum_{i=1}^{m} log \\sum_{z^{i}} Q_i(z^i) \\frac{P(x^i,z^i|\\theta)}{Q_i(z^i)}$$然后引入凸函数的Jsen不等式，即f(x)满足 f’’(x) &gt;= 0，即为严格凸函数，有不等式如下$$E[f(x)] &gt;= f(E(x))$$则$$L(\\theta,z) = \\sum_{i=1}^{m} log \\sum_{z^{i}} Q_i(z^i) \\frac{P(x^i,z^i|\\theta)}{Q_i(z^i)} \\\\\\geq \\sum_{i=1}^{m} \\sum_{z^{i}} Q_i(z^i) log \\frac{P(x^i,z^i|\\theta)}{Q_i(z^i)}$$这样，$L(\\theta,z)$就有了一个下界，我们的任务先固定$\\theta$来找一个合适的$Q$，整体的思路就是先让下界=L,然后固定Q，在去优化$\\theta$让下界最大化。Jsen不等式成立的条件就是f(x)式常数，于是$$\\frac{P(x^i,z^i|\\theta)}{Q_i(z^i)} = c \\\\P(x^i,z^i|\\theta) = c Q_i(z^i) \\\\\\sum_{z} P(x^i,z^i|\\theta) = c \\sum_{z} Q_i{z^i} \\\\\\sum_{z} P(x^i,z^i|\\theta) = c \\\\Q_i(z^i) = \\frac{P(x^i,z^i|\\theta)} {c} = \\frac{P(x^i,z^i|\\theta)} {\\sum_{z} P(x^i,z^i|\\theta)} \\\\= \\frac{P(x^i,z^i|\\theta)} {P(x^i|\\theta)} \\\\= P(z^i|x^i,\\theta)$$即通过将下界=L找的Q就是基于x和$\\theta$的分布,这个公式推导就是证明了Q的确定就是通过x和$\\theta$的分布来确定的，这个值让下确界等于目标L，然后M步再固定Q取优化这个下确界就行了，当然是可以证明每次迭代都会比上一次更好，这里不赘述了。","categories":[],"tags":[]},{"title":"boosting tree总结","slug":"boosting_tree","date":"2019-10-15T15:45:43.000Z","updated":"2020-08-01T01:02:00.000Z","comments":true,"path":"2019/10/15/boosting_tree/","link":"","permalink":"http://yoursite.com/2019/10/15/boosting_tree/","excerpt":"","text":"回顾一下boosting tree的基础知识，主要是gbdt、lambdamart、xgboost、lightgbm等，温习一下主要的实现原理,keypoint，这里对公式啥的就简单的表示一下。之前的文章中提到了决策树，决策树的目标是又多种定义方式，比如信息增益、信息增益率或者gini系数，决策树是单颗树，而boosting是用多个弱分类器来拟合最后的得分，即F(x) = f1(x)+f2(x)+…+fn(x)，fi(x)即弱分类器，实现上用tree来实现，所以现在的问题就是给你一个训练集，Train=[1…N][x1..n]，如何来训练这么多tree，叶子节点的value代表最后的分数,区别于random forest的bagging，即tree之间的训练没有明显区别，这里是boosting，即提升。既然是tree，如何控制分裂？分裂的最大收益是什么？ gbdtgbdt分裂的标准是”统一、均匀”，拟合label的任务由loss的一阶导数来做，即tree分裂后，落到一起的样本”loss的一阶导数”的均方差最小，则为最佳分裂点。叶子节点上的值也是loss对F(x)i-1的求导(注意这里不是对fi(x)的求导，每次完事后需要更新). Fi(x) = F(x)i-1 + shrink$\\cdot$fi(x)，当然叶子节点的值没有shrink，loss使用最简单的最小二乘法loss=(yi-Fi-1(x))2，可见这个loss是可导的，这个非常重要，对 F(x) 求导得 gradient=-2$\\cdot$(-1)(yi-Fi-1(x)), gradient的就是所求的fi(x)很简单(拟合残差，最后加一起可不就是label了么)，可以通过参数设置树的深度、叶子节点最大个数、均方差满足提前收敛等。缺点是没有设置模型复杂度目标不是么？应该是可以做的， Obj = EM(gi-$\\overline{g}$)2+W2(W是分裂点的叶子权重f(x)) + lambda$\\cdot$T(T为页节点个数) [这应该就是防止无限分裂以及某些点上出现极值的情况发生], 每次遍历所有的feature的所有value split，得到 min(Obj)，即使最佳分裂点，当然不能比没分离的还挫。 lambdamart肯定会有问题，这个gbdt是否只是point-wise，如果扩展到 pair-wise和list-wise，好像有困难，pair-wise和list-wise的一些标准计算再算loss貌似不可导，如果引入group的概念来解决组内的一些规则的计算呢，比如搜索场景下一个query对应多条结果，算MAP, ERR, NDCG等，把这些搜索指标融入到树的分裂目标？这里就提到了lambda mart，之前先介绍rank-net，引入了pair对比信息，定义了$$Sij=\\begin{cases}0 &amp; x_i负例，x_j正例 \\\\1 &amp; x_j负例，x_i正例\\end{cases}$$于是定义了目标$$C=\\frac{1}{2}(1-Sij)\\delta(s_i-s_j)+log(1+e^{-\\delta(s_i-s_j)})$$这样，目标里就简单的将两个文档的比较融入到目标中，比如上面的Sij目标中，xi是正例，xj是负例，那么拉大si和sj的差距成为目标, C对w的求导可以转化为C对si和sj的求导！而！！容易得到$$\\frac{\\partial C}{\\partial s_i} = - \\frac{\\partial C}{\\partial s_j}$$这样第i步的迭代即f(xi)的计算可以转化为组内和其不同label的样本的计算，比xi差的为其贡献正梯度，比xi好的为其贡献负梯度，这就是 $\\lambda_{ij}$当然这远远不够，这仅仅是rank-net，这里大家看到的好像就是两个pair之间的定制关系，如何引入复杂的目标呢？ 比如NDCG, 很简单，$\\lambda_{ij}$*($\\Delta$NDCG)，$\\Delta$NDCG 这个怎么算？交换i和j后整个NDCG得分的bias。好了，lambda-mart诞生了，rank-net提供了复杂应用目标的梯度计算方式，mart即多个弱分类器(tree)提供了整体的框架，那就结合吧。最后叶子节点值的结算可以参考论文里的，直接带进去算就可以了，当然要记住，分裂的基础仍然是拿这个f(x)做均方差分裂, 将均匀的样本分到一起。Lambda-mart-gbdt 的实现分裂方式不变，仍然采用均方差最小的方式(暂不提正则)，而叶子节点的计算不再是简单的loss的负梯度，而是转为lambda rank的计算方式，通过group里lambda的方式获得，这里又不仅限于lambda，而是进一步优化，采用牛顿法的思想，一阶梯度不行了，要对Ft-1(x)做二阶导，$F(x)=F(x)-F’(x)/F’’(x)，f(x) = F’(x)/F’’(x)$, 二阶导数有利于更快的收敛，这也是优化点之一。 xgboostxgboost 呢，对目标做了详细规划$$OBJ=L+\\Omega \\\\L = L(yi, Fx-1(x)+fi(x)) \\\\\\Omega = T + \\frac{1}{2} * \\lambda * W^{2}$$这里 对L做了泰勒二阶展开(万物皆可泰勒)$$f(x+\\Delta x) = f(x) + f’(x) * \\Delta x+\\frac{1}{2} * f’’(x)+(\\Delta x)^2$$算出gi, 和 hi, 即Fi-i(x)的一阶导和二阶导，$\\Delta x$ 是叶子节点的权重即fi(x)即w。通过归并，将训练集归并到T个叶子节点上，通过求最大值，可以得出最小值，这个最小值则是split的参考, gi和hi可以很容易的通过同group里不同label的s算得到。这里的split和上面的不一样了，这里的split是让L最小的split，即遍历各个split value，根据泰勒展开后满足条件的$\\Delta x$，带入求得最小的min$\\Sigma$L。由于是二阶泰勒展开，所以也是和牛顿法有相同收敛速度的算法。参考陈天奇的 “Introduction to Boosted Trees” slide Light GBMhttps://www.hrwhisper.me/machine-learning-lightgbm/ 不错的介绍 特点 直方图:Float feature =&gt; bin 分桶里，len(bin) &lt;&lt;&lt; len(feature)，split点的计算量不再随sample的增加而增加 Presort改为pre cal，将每个sample的g,h 计算出来，落到分桶里；计算best split时，可以根据直方图，直接计算，计算量减半，right = all - left，且直方图可以通过简单的减法，在分裂时直接继承 Leaf wise: 不再限定死宽度，允许深度分裂，较灵活 GOSS: 单边采样，相对于列采样，效果更好？ada的思想，对拟合不好的样本加大其对应梯度的权重 EFB: exclusive feature bundling，互斥的特征捆绑，降低特征维度，（不采用PCA的原因是PCA是有损的。。。）， EFB可以比较精细的控制，如果完全互斥，则肯定对精度无损，如何获取互斥特征，采用图着色法，相同颜色的节点(特征)则可以绑定 gbdt里的目标逼近$F(m)=F(m-1)+f(x)$，如何通过优化$f(x)$来让$F(m)$逼近真实的目标y？首先定义损失函数，最简单的就是$loss=|y-F(m-1)|$，由于绝对值不好求导，所以一般转为最小二乘法的形式，即$loss=(y-F(m-1))^2$，通过改变自变量$F(m-1)$来获得目标loss的极值，肯定是要对自变量求导，用负梯度作为方向来更新自变量，所以$$f(x)=\\vartheta_{F(m-1)}loss$$这里有个问题，最小二乘法可以用牛顿求解么？可以先看一个例子 $y=x^2$，初始x=2，求y极值对应x的变化，$loss=0-x^2$，$g1=-2x$，如果按照这个更新，step2中，$x=2+2(-2)=-2$，可以看到一阶的倒数可能方向是ok的，但是步子容易迈得太大，需要learning rate的控制，如果进一步使用牛顿法呢？$$x=x-\\frac{g1}{g2}$$$$g2=\\vartheta_{x}g1=-2$$这样$x=2-(-22)/(-2)=0$，似乎更好一些？我是认为牛顿法适用于所有类型的loss，而并非简单地就不好使。好了，下面开始说二分类的目标，要复杂一些。先直接贴结论:$$f(x)=-g1=\\frac{2y}{1+e^{-2yF(m-1)}}$$ 而叶子节点的值为$$\\gamma_{jm}=-\\frac{g1}{g2}=\\frac{\\sum_{x_i\\in{Rjm}}f(x_i)}{\\sum_{x_i\\in{Rjm}f(x_i)(2-f(x_i))}}$$","categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"}]},{"title":"深度增强学习实现实例(atari)","slug":"dqn","date":"2018-03-08T10:51:28.000Z","updated":"2018-06-08T14:51:26.000Z","comments":true,"path":"2018/03/08/dqn/","link":"","permalink":"http://yoursite.com/2018/03/08/dqn/","excerpt":"","text":"前言本文主要是纪录近期实现的一个增强学习的实例，一直再说增强学习，原理大家也都理解，可是受制于图形学、算法等，直观的增强学习的效果还是很难看到的，多亏了ale，结合opencv可以直观的看到增强学习的效果，下面将详细介绍DQN实现atari在mac上的实现。 参考资料 当然是参考经典的论文Playing Atari with Deep Reinforcement Learning基本实现github:https://github.com/gliese581gg/DQN_tensorflow ALE环境介绍小时候都玩过射击、乒乓球等简单小游戏，其实就是atari小游戏，就是控制左右上下等来躲避怪物，具体的可以参考http://www.arcadelearningenvironment.org/，现在比较好的就是这个平台开源了，可以在电脑上来模拟这些小游戏，可以通过api的方式输入action来控制，输出下一帧的图片以及reward和是否结束等，reward可以灵活设置，比如没有terminal就是1，terminal了就是0。 如何安装 git clone https://github.com/mgbellemare/Arcade-Learning-Environmentmkdir build &amp;&amp; cd build; cmake -DUSE_SDL=OFF -DUSE_RLGLUE=OFF -DBUILD_EXAMPLES=ON ..;make -j4sudo python setup.py这样就安装了ALE的python版本。 如何使用ALE？需要安装cv2,并且下载一个游戏的rom文件，这里用breakout来做实验。大概的实现如下代码。12345678910111213from ale_python_interface import ALEInterfaceimport cv2ale = ALEInterface()ale.loadROM(&apos;roms/breakout.bin&apos;)screen_width, screen_height = ale.getScreenDims()numpy_surface = np.zeros(screen_height*screen_width*3, dtype=np.uint8)ale.getScreenRGB(numpy_surface) #这样就获得了开始的游戏图片界面legal_actions = ale.getMinimalActionSet() #获取该游戏合法的操作输入，比如 0=&gt;left, 1=&gt;rightcv2.startWindowThread() #开启图形渲染线程cv2.namedWindow(&apos;breakout&apos;) #图形框名字ale.act(action_indx) # 输入操作指令，下次getScreenRGB就会获得该至指令下的下一帧图片cv2.imshow(&apos;breakout&apos;,numpy_surface) cv2.waitKey(25) #图形窗口展示，25ms刷新一次 论文结构qlearning强化学习中有状态(state)、动作(action)、奖赏(reward)这三个要素。训练的主体需要根据当前状态来采取动作，获得相应的奖赏之后，再去改进这些动作，使得下次再到相同状态时，智能体能做出更优的动作。这个过程就是qlearning，这其中的核心就是q表的训练，q表就是状态、动作和奖赏的映射表，处在某个状态中，该选择哪个action，直接查表即可，当然这时候的q表已经非常准确了。m个状态，n个action，对应的reward表可以表示如下所示： s/a A1 A2 … An S1 R11 R12 … R1n S2 R21 R22 … R2n … … … … … Sm Rm1 Rm2 … Rmn 一般例子里用小孩学习还是看电视，最后做推理，那个里面看电视和学习就是两个action，状态就是有限的几个状态。flappybird里的action就是上下，状态可以用小鸟距离前一个下桶，上桶的距离以及地面的距离这三个值来作为状态衡量，而atari里action显然就是玩游戏的那几个手柄键(最多18个action)，状态是什么呢？状态就是原始的图片，所以这是个非常庞大和复杂的Q表，通过输入状态(原始图片信息)和action查到Q值，所以这是个训练函数的过程，可以通过svm等线性模型实现，这里我们使用深度神经网络来模拟这个函数，就是论文中提到的dqn。 loss与learningQ表里的是q值，即收益值，该值与当前的直接reward以及下一个状态最大值有直接关系，reward表示当前状态下的收益，游戏里可以就表示两种状态，生存或者死亡，0和1即可。表示如下：12self.discount = tf.constant(self.params[&apos;discount&apos;])self.yj = tf.add(self.rewards, tf.multiply(1.0-self.terminals, tf.multiply(self.discount, self.q_t))) qlearning中的Q值可用函数表示成 $ Q(s,a) = \\max \\limits_{a} {r + \\gamma maxQ(s’,a’)|s,a} $ 结合代码来看，下个状态的最大值通过replay来获取，即上一个参数下(老的theta), 该状态下的最大值q_t，这样就得到了相当于supervised里的label Q(s,a)，与旧的函数里算的Q值做差值即产生了loss, 其实可以看到状态的reward再不断的学习进来，并且未来的前途也会考量，但是最终都是由本地的reward这个实实在在的东西得来的，不死即是目标。 q_t细究q_t是当前状态下一阶段(给定的action到达的下一状态，在online时已经发生的状态，不一定时最有的action)的最大收益值，通过older的网络在online时尝试各种(action)后获得。如下的计算网络Loss(用于更新网络参数)可以看出，yj是label，Q_pred是older网络(当前网络，更新参数前)根据action算出的值。12345self.discount = tf.constant(self.params[&apos;discount&apos;])self.yj = tf.add(self.rewards, tf.mul(1.0-self.terminals, tf.mul(self.discount, self.q_t)))self.Qxa = tf.mul(self.y,self.actions)self.Q_pred = tf.reduce_max(self.Qxa, reduction_indices=1)self.diff = tf.sub(self.yj, self.Q_pred) replay一般的增强学习里都是需要replay，棋类游戏的自我对弈也是获取replay的方式，简单的游戏里状态的确是有限的，比如象棋，但是像围棋这种状态很多，大概$2^{19*19}$，action又较多361，象棋要少很多，这种情况下，罗列状态做成map映射是不可能的，可以采用深度神经网络的方式来处理原始状态的情况，但是label比较难获取，如何自我训练获取label?这里就用到replay机制，将Q(S,a)分解成当前的可看到的reward以及下一个状态最大的Q值(可以用旧的网络直接计算)，下一个状态如何获取？如果遍历当前所有的action显得比较笨重，这样体现replay的用处了，先跑一遍，在回头来一下，所以是off-policy。为何不是on-policy？我的感觉主要是随机，因为当下训练的batch就是随机抽出来的，而非on-policy实时关联的，实时policy会导致问题，”For example, if the maximizing action is to move left then the training samples will be dominated by samples from the left-hand side; if the maximizing action then switches to the right then the training distribution will also switch. It is easy to see how unwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically”。online通过$\\epsilon -greedy$的方式获取较均匀的状态，这里不表。 DQN架构详解DQN的实现是使用tensorflow，输入是当前的图像特征、当前状态的奖励、当前状态是否是终止状态、下一步的action以及action对应的下一个状态的最大收益，最大收益是通过replay的方式可以直接获取的，也就是之前提到的$\\theta _{i-1}$。通过tensorboard将基本的构图画了出来，如下所示：整体架构，又输入、卷积层、全连接层、loss、train目标层构成三个卷积、relu如下所示：两个全连接层如下：loss层如下： 训练效果github中给出了一个链接，https://www.youtube.com/watch?v=GACcbfUaHwc，基本上就是这个效果，训练了两天以上后，我自己也训练了一个版本，不过只训练了一个晚上，效果已经非常不错了，感觉已经超过我的水平了，大体的方向性还是很明显的。","categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"},{"name":"ml","slug":"ml","permalink":"http://yoursite.com/tags/ml/"}]},{"title":"换工作小结","slug":"job","date":"2017-06-27T13:59:24.000Z","updated":"2017-06-28T10:07:36.000Z","comments":true,"path":"2017/06/27/job/","link":"","permalink":"http://yoursite.com/2017/06/27/job/","excerpt":"","text":"背景北漂快五年了，什么都没有，房、车、户口，甚至连工作居住证都懒得办，这么懒、这么挫的后果就是在北京待不下去了，毕业后就一直在360这家公司，说实话，学到的东西不多，在外面看机会悲剧的多，当然也与我自己方向捏不清楚有关，截止到现在可以说是较团圆的结局，如愿的拿到了杭州阿里的工作机会，北京这边也有几个机会，接下来我会分享这一个多月的历程，充满了辛酸和苦辣，不到最后一个心都放不下来的感觉。 准备知道了17年4月份可能会找工作，所以肯定会提前准备的，刷了一些leetcode的题目，在平时的项目中特别留意了便算法的地方，比如ltr的系统学习，通过ranking组的一些项目，wiki以及主要算法的paper等，实践方面在问答引擎的ranking上使用ltr算法。不过还是有些滞后，准备的不太充分吧，直到5月初才开始正式找工作吧，找了一个猎头，内推的内推。总结一下5月份之前的准别: leetcode review了100道题，覆盖了树、排序、dp等面 问答项目使用ltr，算是有了算法类项目 简历cover了两个新项目，除了问答ltr还有通用引擎 算法总结:svm, lr, 决策树, ranklib, ltr, gbdt, xgboost等 xgboost源码阅读 深度学习算法，实现了一个CNN网络模型 tensorflow学习 出师不利开始让波波桑帮忙投了头条，没想到会这么快，头条算是看的比较重的一个机会，去年就栽在上面了，这次又是第一个面的，感觉不太安全，果真，第一面还比较顺利，问了下项目，主要是问答的ltr项目，主要就是标注集，特征，效果等，还让我说了gbdt算法的原理，答得不是很好，回答了分裂方式，以及叶子节点值，我记得当时我是说了叶子值是二阶导数，这个是扯淡了，一定是看混了。然后还提到了微软的lightGBM，这哥们说效果一般，后面又考了一道算法题，问一个二叉树是否是另一个二叉树的子集，这个问题回答的较顺利，后面基本上就进入二面了。第二面是真正的克星，最怕这样的面试官，开始就问问答ltr的一些痛点，主要是问点击特征，如果有偏差，积累效应怎么办？我当时是有些慌，因为之前组内分享的一个关于rank实验的东西我就没好好理解，我以为是通过这个来将，结果就各种乱，思路也很差，让面试官很不满意，后面就随意出了一道题，n个分布，各种feature在一个矩阵中，然后用贝叶斯公式求一个概率，这个写起来是没有问题，但是最后又让我实际编程求出具体的平均值和方差，写就写呗，较长，最后是成功写出了，感觉应该是一个加分项，可是这时候面试官其实心理早有定论，只是在走过场了，问了我如何在不求平均值的情况下求方差？至今没有答案，后面草草的问了下正态分布的公式？？黑人问号，我写了后面的那一串，前面的系数忘了，然后就没了，让我问问题，我天真的以为自己答的还不错，还期待加入他们，晕，丢人。后面漫长的等待，我似乎很有信心，在简历的背面详细的记录这本次的心的，信心满满的等待接下来一轮的面试，结果结果，等了20分钟，二面的人又回来了，说出了我最不想听的话，真是很很失望，本来答应和女朋友去周边逛逛的我毫无兴致，一句“一面面试官还是很欣赏你的”在我脑海里挥之不去，走之前见了下波波桑，和他大概说了下本次过程，头条真是个不错的机会，待遇也不错，可是我仍然栽了跟头，打击真是太大了，悻悻的回去了，跟失了魂一样。5月7日周日的下午。 外企这个是猎头推荐的，其实开始我是想去知乎的，但是知乎不招人，简历直接就刷了，人家只招leader，还说这简历上的人发展不到我们的要求，，晕。。。然后这个猎头就推了与他们合作较多的freewheel，一家做视频广告投放的公司，HR很热情，5.8日周一就加了我微信，下午就通了电话，问了我现在的情况，没有聊薪资，我不想说，因为offer都没呢说这个有啥用，后面就安排了面试时间，先是一个电话面试，安排在周四的上午十点半，记得当日真的是很紧张，我是先到公司然后回的家做的电面，因为需要线上coding，回家的路上骑着车痘痕紧张，哈哈，开始了，是个女面试官，首先让我讲一个项目，看出来她是之前没咋看我简历的，无所谓了，就挑了热点挖掘的那个项目讲了一下，电话里，我滔滔不绝(之前准备过，所以比较顺畅)，大概7-8分钟，但是电话那边毫无反应，我有点蒙比，说完之后很明显的感觉到对方是什么也没听懂，服了，然后直接就进入下一个环节，让我编程，是组合硬币到一个目标的程序，说实话，这道题是leetcode题库里的，有非常简洁trick的解法，就是push-pop组合，非常巧妙，我就使用了这个算法，大概用了3-4分钟吧，写出来之后女面试官彻底爆发了，非要说我的是错的，我一解释，发现完全不是一个频道，本身递归这个在电话里不太好一层一层让她看明白，最后我真是急了，真是举例一层一层把递归剥出来希望她能明白，最后她好想也觉得这个算法是对的，然后看了一会，问了几种情况，我接着解释，最后她觉得可能我是对的，但是仍然不确定，所以就差不多得了，我是比较郁闷了，希望能把代码保留下来，让她回去好好看一下，她急忙说不用不用，本来这个题不光考算法，也是考察思路，意思就是我虽然算法对，但是思路不清，说不明白，黑人问号。。然后后面就来了一个english talk，好吧，虽然口语比较次，但是只管说呗，磕磕绊绊，终于讲完了，电面算是结束了。一直等到了第二天才通知我说面试通过，不过HR又给我打了一个电话，聊一下昨天电面的情况，我只好说电话面试的缺点啦，不太好沟通，其实我是对的，我更想说女面试官水平很低，但是我敢吗。。。后面又安排了面试，周三下午，5月17号。不错这次有了冲突，因为我上午约的是美团平台部，内推的，不过我请这个机会一般吧，随便面面，没想到会发生后面的事情，放在后面说，这次就是下午没面成，改成5月23日下午，让我预留一整个下午，整的挺吓人的，公司本部是在亮马桥的一个写字楼，打车过去的，还是比较紧张的，到了之后沙发上深呼吸一下，接下里面试了，一面一个虎头虎脑的码农，先让我介绍项目，我讲了哪个项目忘了，然后又说了他们现在的一些问题场景，就是多个字符串kv查找问题，我让他们使用高效的数据结果，比如darts等，问了下他们每天的hadoop任务，感觉比360的还要低端，更新的也是很慢，可能他没说清楚或者我没有get到，然后考了我一个pthread的一个函数，这个我就黑洞了，多线程c++这块没咋弄过，然后做了一道算法题，链表的k长度反转，这个我答的真是不好，虽然大体写出来了，但是感觉写的时间太长了，思路应该是没有问题的，然后进入下一面，是一个圆脑袋的码农，一进来就是问一个场景，也是用矩阵表达，多标签的问题，各种边界条件，我建议使用位图表示标签，遍历的方式统计，然后各个边界是要考察的点？最后都快成数学证明题了，我感觉没啥应用场景吧，哪个问题会抽象成这样，然后这哥们说他们是有这种场景，我觉他们是跑偏了，我是没有说了，就是跟着他的思路吧。后面又问了一个多进程刷文件的问题，怎么加锁，同步等，应该答得还可以，然后就结束了，我感觉答得还可以，应该下一面没问题，不过这个部门太偏工程了，真没啥兴趣，不过我还是希望能走远点，等了20多分钟，为什么每次都这么久？结果结果等来了HR，我知道不妙了，因为一般面试都得5轮才能见到HR，我知道我悲剧了，情绪很低落，不知道说什么好，没啥说的吧，最后走了，之后和猎头确认的确是下一面的leader不在，不过我感觉没啥机会了，因该真是不合适，这个外企号称待遇好上天，就这样吧，没了就没了，不过我还没有offer，很是着急。 5月23日周三的下午。 美团好吧，终于到美团了，今年的幸运符号，美团可以说是开始最不看重的，但是过程和结果却是出乎意料的顺利，满意，非常感激，真是不敢想没有美团的鼓励，我能否坚持下来。美团是一个快要离职的朋友内推的，推到了平台部，我一看以为是纯工程的，反正去走个过场吧，也是和上面的那个freewheel安排在了一天，上午随便搞搞吗，到了之后hr把我领到了美团的楼上，一面的码农瘦瘦的，还不错，人挺好的，让我说了一个项目，我说的是通用平台，我觉得和他们做的应该有关吧，然后讨论了较多倒排索引的事情，我比较亲怪，平台部也搞搜索引擎？最后问了下 才知道原来这个平台部就是美团的搜索部门，这不巧了么？这块我熟悉所以应该面的挺好的，最后考了算法题，是把有序数组的一段逆序，这个也是leetcode的题，我还专门研究过有相同元素的解法，整个过程非常愉快，顺利到了下一面，一看时间已经11:45了，我感觉太慢了，二面的对机器学习比较感兴趣，问了ltr的一些算法，最重要的是让我推到了xgboost的目标函数，我之前也是较细的研读过陈天奇的slides，所以能够大体写出来，算是还可以吧，太细的就不行了，然后还打听到我用过深度学习做过问答的项目，我就把私下里搞的那个qa的CNN模型跟他说了一下，应该还挺满意的，然后就是算法，是一个整数比大小的最小整数，二进制里0和1的数量保持一致，开始我的思路差点对了，不过还有问题，最后他提点我一下，然后就让我写出完整代码，很顺利，写的没问题，这个面完已经12点半左右了吧，我有点慌了，这次肯定能过，但是下午1:30我还要面外企呢？过了一会来一个应该级别比较高的人，很nice，铭哥，后面还有很多交集，他先带我去吃了饭，我以为是免费的，就狂拿菜，结果一个人拿了28元。。有点惭愧，还就吃了一点点，我是一有事情就吃不下去饭的主，然后这个leader就找我聊了，大概1:00开始聊的吧，哎，这个过程的真的非常nice，这个leader问的东西几乎都是我平时工作时会关注的问题，对集群部署，abtest，ranking等，也诱导我说了很多工作中的想法，感觉非常过瘾，事后也得知这个反馈很好，但是我向他说我下午还有事情，然后就走了，非常的依依不舍，哈哈，然后说后面会联系我，我之前经历过一些面试，一般说后面联系的大多不联系了，但是这次感觉是真的，我也是觉得面试面的不错嘛，所以我坚信这个offer我已经拿了大半了，回去后也没赶上外企的面试，但是这毕竟是一次成功的面试，对我的鼓励作用非常大。铭哥说是明天或者后天联系我，也就是5.18号或者是5.19号，结果到了18号，无任何音信，我有点慌了，感觉不好的兆头又来了，坐不住了，就疯狂的联系美团的hr(给我发面试邀请的)，邮件短信电话都弄了，结果没有任何回音，电话竟然直接挂我的，我服了，难道善良的我再一次被欺骗了？哀莫大于心死，最后我放弃了，算了吧，结结果，第二天突然接到铭哥的电话，我真是激动啊，他说当时面完的当天晚上打篮球他腿就伤了，第二天没上班，所以就没有联系我，哈哈，原来是这样，这真是一道曙光，然后告诉我后面会约我面试，交叉面，定级别的，我好开心，真的是没有骗我，定的是下周二下午，但是那个下午是外企的面试，所以就换个时间换到了周三了，周三下午打个车就过去了，我的水笔在出租车上丢了，我还害怕了一阵，这个笔感觉有点幸运，丢了不会是什么不好的兆头吧，一瘸一拐的铭哥接我上了楼，给我打了简历，后面就是等，那个交叉面试官在开会，我大概等了50分钟，晕，还好终于等到了，那哥们是外卖搜索的nlp负责人，当然主要是考算法，让我挑一个项目给他讲讲，然后就挑了热点挖掘的那个，刚说到模糊化，这哥们就让我说一下怎么模糊化，这个还比较擅长，几乎说全了考虑的因素，最后说到使用什么模型的时候出问题了，我说是使用随机森林，然后就问我为啥使用随机森林，我说这种分类问题比较简单，所以使用随机森林，然后就炸了，何为简单？何为复杂？难道回归是复杂，分类就是简单？我晕，得确我这随口一说真是太不严谨了，然后又说到如何进行多分类，我只看过softMax这种每种分类训练一组参数，实在是想不出一种可以直接分类的算法，还好这哥们是交叉面试官而已，又是校友，他的一个同学还在360搜索，哈哈，有了解到我的算法知识都是自学的，还可以吧，马马虎虎过关，不过因该不是很理想，本来级别能够定的更高点的吧，这个就面了30左右吧，然后我就等最后的老大面吧，等了好久，铭哥告诉我老大还是没有时间，只有先回去了，没事就先回去吧，我信铭哥的，回去后有hr给我联系了，还让我后面接着面试(就是没有赶上的那个老大面)，但是当时快端午节了，各种拖，我也跟她说自己有工作机会，希望流程快一点，最后的最后他们说不用面了，可以谈offer了，应该级别不是很高吧，终于拿到了offer，具体薪水细节不提了，后面由于神马的offer，最终还是放弃了，铭哥对我很好，我也非常感谢他对我的认可，后面聊了许多，以后我们还是朋友吧，我也多了一个职业发展路上的挚友。 转岗除了在外面看机会，我还在公司内部看了机会，因为是想转纯算法的方向，垂直领域做了这么多年还是以工程为导向的，没有什么积累，于个人发展非常不利，公司内部的商业化团队无疑是吸引力比较强的，偏广告方向，不过各种算法和模型使用比较多，如果能够去那里，那也是一次非常好的机会，所以5月初也是向那边的hr投递了简历，希望可以安排与那边老大约的机会。5月9号有了回音，让周四下午五点去和老大聊聊，这个是广告平台投放部门，见到老大，人还是比较nice的，开始问了为何转岗？然后问了问答ltr的项目，然后让我说下gdbt和xgboost的区别，那个时候还比较早，对gbdt和xgboost的了解真的是非常有限吧，这个真的是在找工作的时候深有体会，说到的仅仅是xgboost的数据预处理、正则项等等，现在感觉好浅，然后基本上就是问问题吧，但是当时基本上没啥准备，记得的一个问题就是他们那是否用深度学习，记得说了只用到了NN，然后就匆匆的结束了，其实我应该多准备关于广告的问题的，太匆忙了，负责人也觉得我太草率了吧，后来的后来就是HR来通知我被通过了，其实我觉得和老大聊的真的是很差，通过的原因应该与我在公司的简历相关，我在公司这么久，还算比较出色吧，所以招我风险肯定不大，HR告诉我这个消息时，我还是比较高兴的，憧憬着在新的部分重新开始，贪婪的吸收和享受着广告这个领域的知识和成长，并大展拳脚。当然转岗的流程，周期都是需要考虑的，得到通知的时间是5月24日，HR很热情的和我沟通着后面的流程和注意事项，口头应允的我隐隐的觉得这个机会可能最终会被放弃，由此而来的一个月的拉锯战非常痛苦，的确这个时候处于看机会的高峰期，但是又不能直说，因为之前沟通的时候是说优先优先考虑360，有360的机会肯定不会走的，可是这并非我真实想法，我的终极目标仍然是杭州。最后的拒绝很难开口，一个多月的敷衍终于走到了终点。 阿里神马","categories":[],"tags":[{"name":"life","slug":"life","permalink":"http://yoursite.com/tags/life/"}]},{"title":"rotate array search","slug":"rotate-array-search","date":"2017-04-30T02:07:18.000Z","updated":"2017-04-30T02:56:22.000Z","comments":true,"path":"2017/04/30/rotate-array-search/","link":"","permalink":"http://yoursite.com/2017/04/30/rotate-array-search/","excerpt":"","text":"leetcode上遇到了一个问题，就是如果把一个sorted数组旋转一下，如[0,1,2,3,4,5,6,7] =&gt; [4,5,6,7,0,1,2,3]，这样二分查找还将如何进行？,当然是用O(log(n))的方法。 问题定义应该是如下的场景，12int search(vector&lt;int&gt; nums, int target) &#123;&#125; 不重复的情况这里会有一个问题场景的问题，即数组中是否允许存在重复的元素? 如果不允许，则找到旋转的那个点，如例子中是在value=0处，即idx＝4，找到了这个点的方法有些多，可以使用寻找最小值的方法，也可以直接二分查找，如：12345678910111213int findMinIdx(vector&lt;int&gt; nums) &#123; int lo = 0, hi = nums.size() - 1; while(lo &lt; hi) &#123; int mid = lo + (hi - lo) / 2; if(nums[mid] &gt; nums[hi]) &#123; lo = mid + 1; &#125; else &#123; hi = mid - 1; &#125; &#125; // lo==hi is the index of the smallest value and also the number of places rotated.&#125; 也可以直接二分的算法查找，只不过原来的方法要稍微变动下，这种方法比较高效，不够思路有些不太清晰。1234567891011121314151617181920212223242526272829int search(vector&lt;int&gt; nums, int target) &#123; int start = 0, end = nums.end() - 1; while(start &lt; end) &#123; int mid = start + (end - start) / 2; if(nums[mid] == target) &#123; return mid; &#125; else if(nums[mid] &gt; nums[start]) &#123; if(target &lt; nums[mid] &amp;&amp; target &gt;= nums[start]) &#123; end = mid - 1; &#125; else &#123; start = mid + 1; &#125; &#125; else if(nums[mid] &lt; nums[start])&#123; if(target &gt;= nums[start] || target &lt; nums[mid]) &#123; end = mid - 1; &#125; else &#123; start = mid + 1; &#125; &#125; else &#123; start++; &#125; &#125; return -1;&#125; 重复场景如何？这个没有争议，关键点在于如果数组中允许存在相同的元素，这个算法该怎么写，可以肯定的是上面的算法都将失效，比如[4,4,4,4,0,1,2,4]找不到最小的地方的，还有[4,6,4,4,4,4,4,4]如何找到rotated place是2，很难，经过验证，无法在log(n)的时间内找到翻转的位置，不过可以找到最小的值是多少，至于在哪个位置就不确定了，即最多找出一个最小值的idx。1234567891011121314151617181920212223242526272829303132333435int findMinValue(vector&lt;int&gt; nums, int start, int end) &#123; if(start &gt; end) &#123; return -1; &#125; int mid = start + (end - start) / 2; while(start &lt; end) &#123; if(nums[mid] &gt; nums[start]) &#123; int pos = findMinValue(nums, mid + 1, end); if(pos == -1) &#123; return start; &#125; return nums[start] &lt; nums[pos] ? start : pos; &#125; else if(nums[mid] &lt; nums[start]) &#123; int pos = findMinValue(nums, start, mid - 1); if(pos == -1) &#123; return mid; &#125; return nums[pos] &lt; nums[mid] ? pos : mid; &#125; else &#123; //相等的情况较为复杂, 两边都要考虑 int pos = mid; int pos1 = findMinValue(nums, start, mid - 1); if(pos1 != -1) &#123; pos = nums[pos] &lt; nums[pos1] ? pos : pos1; &#125; int pos2 = findMinValue(nums, mid + 1, end); if(pos2 != -1) &#123; pos = nums[pos] &lt; nums[pos2] ? pos :pos2; &#125; return pos; &#125; &#125; return -1;&#125;","categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"},{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]},{"title":"基本的决策树算法总结","slug":"base-dt","date":"2017-03-20T11:34:43.000Z","updated":"2020-07-03T15:48:58.000Z","comments":true,"path":"2017/03/20/base-dt/","link":"","permalink":"http://yoursite.com/2017/03/20/base-dt/","excerpt":"","text":"这篇文章仅仅是简单的回顾一下决策树开始的一些经典算法，虽然较旧了，用的人少了，但是一些思想和算法，值得了解。下面主要介绍的算法有ID3、C4.5、CART、SLIQ、SPRINT，其中CART算法一笔带过吧，有些需要深究的部分，我也没有太理清，就把我现在理解的东西权当是记录一下把。 ID3ID3(Iterative Dichotomiser 3)，迭代的二分法分离模型version 3，是根据信息增益来做的决策，所以如何定义信息增益是个主要的问题？首先来看一下信息熵的定义:$H(S) = -\\sum_{i=1}^{m} P(u_i)logP(u_i)$其中$S$是样例的集合，$P(u_i)=\\frac{|u_i|}{|S|}$，一般认为越有序的数据熵比较低，越乱序的则熵比较大，显然我们分类的目的就是有序，减少熵。最后的信息增益为:$Gain(S,A)=Entropy(S) - \\sum_{v \\in Value(A)}\\frac{|S_v|}{|S|}Entropy(S_v)$遍历所有属性即可，这个算法优点就是简单，奥卡姆剃刀原理，缺点也较明显，抗噪音不行，无剪枝，最重要的是倾向于分类较多的节点分裂，这样其实没有太多意义，于是引入了C4.5算法。 C4.5C4.5是对ID3的改进，因为ID3分裂出的树不是二叉树，而是有多少属性就有多少分叉，所以如果分叉过多，容易导致各个分支上的信息熵为0，但是这种明显不是我们想要的，所以这里提出了信息增益率的概念，公式为:$GainRatio(A) = \\frac{Gain(A)}{SplitInfo(A)}$分母是训练集关于A的分布情况，显然，越多越大，最后的增益率就会下降。选择具有最大增益率的的属性作为分裂属性。有点类似于后面的正则的概念，即SplitInfo(A)是正则项，如果分裂过多，这个值会弱化Gain(A)的影响。SplitInfo(A)的定义如下:$$SplitInfo_{A}(D) = -\\sum_{j=1}^{v}\\frac{|D_{j}|}{D}*\\log_{2}(\\frac{|D_{j}|}{D})$$ CARTCART和C4.5类似，不过是使用gini算法计算收益，gini值越小，效果越好。接下来会讲一下gini算法的实现。 gini基尼指数原来是是衡量贫富悬殊程度的标量。它的定义如下：我们首先收集社会上每一个人的总财富额，把它从少至大排序，计算它的累积函数（cumulative function），然后便可绘出图中的洛仑兹曲线（Lorenz curve）。图中横轴是人口比例的累积分布，竖轴是财富比例的累积分佈。$A$和$B$是图中两个面积，基尼系数为$\\frac{A}{A+B}$越不均匀，比如钱都集中在少数人手里，则$B$趋向于0，gini系数很大，python简单的实现如下:123456789import numpy as npdef gini_coef(wealths): cum_wealths = np.cumsum(sorted(np.append(wealths, 0))) sum_wealths = cum_wealths[-1] xarray = np.array(range(0, len(cum_wealths))) / np.float(len(cum_wealths)-1) yarray = cum_wealths / sum_wealths B = np.trapz(yarray, x=xarray) A = 0.5 - B return A / (A+B) 应用到具体分类的gini定义如下:$gini(S)=1-\\sum_{j=1}^{n}p_j^2$$p_j$为类j出现的概率，如果集合分成$S_1$和$S_2$部分，则分割后的gini为$gini_{split}(S)=\\frac{n_1}{n}gini(S_1)+\\frac{n_2}{n}gini(S_2)$则$GAIN_{gini} = gini(S) - gini_{split}(S)$，越大收益越高。 gini系数收益的补充 上面提到的公式 $\\sum_{j=1}^{n}p_j^2$很难和定义中的积分联系起来，及时联系的话$p_j$应该是越分散越好，和积分对应，分散说明积分面积大，gini系数越小，而这里明天是$p_j$越大约不分散好？怎么对应呢？比如”p1=0.1, p2=0.1, p3=0.8”是比”p1=0.2, p2=0.2, p3=0.6”好的，可是按照积分来看，后面的明显积分面积更大？其实这里的误区是$p_j$不是横坐标对应的值，而应该是横坐标，即横坐标的范围，这里的积分即面积对应的是$p_j$*value，value取多少呢？这里就取得是$p_j$，这样就统一了。试想，如果国家有90%的人年收入超过1亿，那才好呢插一下，国际惯例把0.2以下视为收入绝对平均，0.2-0.3视为收入比较平均；0.3-0.4视为收入相对合理；0.4-0.5视为收入差距较大，当基尼系数达到0.5以上时，则表示收入悬殊。而我国的gini系数可能超过了0.7? 如何收益大？说到gini系数，其实有点相悖？gini系数中如果财产都在一个人手里，那岂不是显得很”纯”? 其实这种情况的确是比较符合理想分类的情形，因为99%的都是穷人，1%的才是富人，不过这样想就错了，因为负例太负了，负例产生的波动太大了，因为太大了，造成了整体的均值不回太低，所以其它99%的产生的mse也是比较大的，其实可以这么理解，一共100个样本，和就是1，如果均匀分布即大家都是同类人，这样才是分类的最佳情况。 SLIQsuper-vised learning in quest, 这个名字的确挺蛋疼的，应该叫一种可伸缩的分类器，这个算法有什么特点呢？如下总结| BFS| 直方图| 属性表| 类表每个树节点都拥有了直方图(local)，属性表(global), 类表(global), 这样也才能使用BFS，特别每个树节点都拥有自己的直方图，可以统一扫一下属性表，然后整体更新树节点上的直方图，直方图如下表所示，假设只有两个分类C1,C2：参数|描述 stats1 C1 C2 left 1 0 right 3 4 这个是最初的统计，其实实际在遍历中，可以用最大收益代替，并附上最大收益对应的attribute以及split_value，当然一般的算收益仍然是使用gini算法。 stats2 Gain Attr Value stats 2.5 a1 6 每次扫描完毕，这当前所有的叶子节点(active nodes)上的直方图都会更新一次，决定了下一次的分裂, 如上面节点中的最佳分裂的属性是a1, $split_value=6$。优点很直观啊，全局一次排序。补一下直方图、属性表、类表。 SPRINTscalable parallelizable induction(归纳) of decision trees, sprint算法，强调了并行，是在SLIQ的基础上更进一步, 将类图合并到每个属性表中，单独存入节点中，属性表随着节点的分裂也被分割，所以就不存在BFS了，而是深度优先遍历的方式，每个节点拥有自己单独的属性表(每个属性都需要一张(sorted))，可以单独分裂，这个可以加快并行，当然存储最佳分类的还是要使用直方图，这个算法的缺点就是每次分裂，需要设计到非分割属性表的元素分裂，需要通过rowid(hash?)对应上，分割相对复杂。 直方图如何对应xgboost?分布式加权直方图算法,这是xgboost里带的，为了解决数据无法一次载入内存或者在分布式情况下算法， 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。直方图补充微软新出的LightGBM算法也是用到了直方图，的确，这个直方图和SLIQ以及SPRINT的直方图并无区别，直方图直接在gbdt中使用，的确起到了比较好的效果，不过相对于level-wise，每个节点都需要维护自己的数据，各个feature上sort的数据，比较麻烦，不过这样的好处就是deep-wise，脱离了深度的限制，另外，LightGBM中使用了bin value作为划分根据，即将feature value专为bin value，一般一个字节，即最多256种划分，这样的好处就是存储压力变小，另外不必扫描所有的feature value，只需扫描256次即可，还有一个就是数据分割时较快速，因为内存占用少，共享索引表，直接划分，还有做差的方式，即处理好一个分裂节点，另一个节点直接通过与父节点作差即可得到，速度又得到了提高。 剪枝算法分为预剪枝和后剪枝 预剪枝1) 树高度2) 叶子节点的纯度以及叶子节点总数 后剪枝只说一种吧，用验证集自底向上看节点的拟合率","categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"}]},{"title":"CNN在问答领域识别中应用","slug":"qa-cnn","date":"2017-03-08T10:51:28.000Z","updated":"2018-03-03T04:49:39.000Z","comments":true,"path":"2017/03/08/qa-cnn/","link":"","permalink":"http://yoursite.com/2017/03/08/qa-cnn/","excerpt":"","text":"背景短文本question-answer匹配度识别问题，论文(Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks.pdf), github实现。github的作者基本上实现了文章里的算法，计算的精度也差不多，不过使用theano实现的，我准备使用tensorflow实现，并解决大规模预料训练太慢的问题。当然本文要开始从分析git的实现开始讲起，会穿插theano的一些语法，实现等，我自己也是从零学起的，且当给自己留个笔记吧。 架构？不能不提到的论文中经典的架构图。网络用文字梳理的结果如下: question answer并行处理成词向量 词向量与卷积核做运算得到n个len(q or a)维的向量 对2中得到的向量做Activation和Pooling处理，得到1维数组，大小为n 问题和答案抽出的1维数组分别与相似度矩阵M做乘积预算，得到相似度，即 $A*M*Q=sim$ 计算answer和question中的相同部分F，将A,sim,Q,F拼接成新的向量 将5中得到的向量做线性变化，即 $\\alpha(W*X+b)$ 最后使用Softmax对6中得到的linear之后的向量做分类 使用theano的实现作者的实现有一些改动，如第5步骤中计算得到的交叉部分F，这部分可以是交叉词的词向量，也可以是交叉词的idf，比重等，显然idf和比重更容易让人信服，不过这个值不好处理，作者并没有用，而是直接使用的交叉词的词向量拼到question和answer的词向量中，即将这部分直接放到开头做了。 数据预处理(embedding)这部分主要是将question-answer对分词(英文就是空格直接分词), 然后根据word2vec，将问题和答案分解成word2vec的词向量，并收集全集词表，word2vec缺失的词使用默认值填充。难点在于训练集合的选取，这是有监督的学习，对于中文的海量语法来说，需要的训练集合的规模也是非常大的。 Load主要load的数据有train, dev, test三类，train是训练参数使用，dev是验证参数，test是实测使用，其实dev和test可以归为一类，每类数据分为question, answer, overlap。 词向量转化LookupTableFast将question, answer, overlap转化为词向量。这里会讲pad部分(即卷积的宽度补上)，最后输出的是 [batch_size, 1, 2 * (q_size + q_filter_widths - 1), ndim], ndim是词向量的宽度(word2vec生成的词向量的size)，后面ndim的长度会拓展为12# sentence长度 + 交叉长度ndim = vocab_emb.shape[1] + vocab_emb_overlap.shape[1] 123456789101112131415class LookupTableFastStatic(Layer): def __init__(self, W=None, pad=None): super(LookupTableFastStatic, self).__init__() self.pad = pad self.W = theano.shared(value=W, name=&apos;W_emb&apos;, borrow=True) def output_func(self, input): out = self.W[input.flatten()].reshape((input.shape[0], 1, input.shape[1], self.W.shape[1])) if self.pad: pad_matrix = T.zeros((out.shape[0], out.shape[1], self.pad, out.shape[3])) out = T.concatenate([pad_matrix, out, pad_matrix], axis=2) return out def __repr__(self): return &quot;&#123;&#125;: &#123;&#125;&quot;.format(self.__class__.__name__, self.W.shape.eval()) 卷积定义Conv2dLayer来做卷积类使用，卷积的初始化如下12345def __init__(self, rng, filter_shape, input_shape=None, W=None):# @rng: 初始化种子# @filter_shape: 卷积核的样式，filter_shape = (nkernels, num_input_channels, filter_width, ndim), nkernels = 100, num_input_channels = 1, 可见与question长度无关。# @input_shape: 同上一层词向量层的输出# @self.W 也和filter_shape具有相同的shape，随机化一些数字 卷积使用库函数，调用如下1234def output_func(self, input): return conv.conv2d(input, self.W, border_mode=&apos;valid&apos;, filter_shape=self.filter_shape, image_shape=self.input_shape) 看看conv2d函数的返回, 看到output的是 [batch_size, filter_size, sentence_size, ndim]12345678910Parameters: input (dmatrix of dtensor3) – Symbolic variable for images to be filtered.filters (dmatrix of dtensor3) – Symbolic variable containing filter values.border_mode (&#123;&apos;valid&apos;, &apos;full&apos;&#125;) – See scipy.signal.convolve2d.subsample – Factor by which to subsample output.image_shape (tuple of length 2 or 3) – ([number images,] image height, image width).filter_shape (tuple of length 2 or 3) – ([number filters,] filter height, filter width).kwargs – See theano.tensor.nnet.conv.conv2d.Returns: Tensor of filtered images, with shape ([number images,] [number filters,] image height, image width) 何为卷积？如何卷积?卷积为分时加权，在词向量上表现为分段计算作为权重，粒度更细了，input_shape细化为单个qa，即为如下格式(加上filter_width)。引用某乎上对卷积的解释: cnn的核心在于卷积核，其实关于卷积核还有另一个名字叫做滤波器，从信号处理的角度而言，滤波器是对信号做频率筛选，这里主要是空间-频率的转换，cnn的训练就是找到最好的滤波器使得滤波后的信号更容易分类，还可以从模版匹配的角度看卷积，每个卷积核都可以看成一个特征模版，训练就是为了找到最适合分类的特征模板。 卷积核最后是作为最终的feature，每个卷积核作为一个特征，与question、answer等都无关了，所以卷积核的特征采集是非常重要的。 $$S =\\begin{vmatrix}| &amp; | &amp; … &amp; | &amp; | \\\\filter &amp; w_1 &amp; … &amp; w_{|s|} &amp; filter \\\\| &amp; | &amp; … &amp; | &amp; | \\\\\\end{vmatrix}$$$S.shape = [ndim, 2 \\cdot (filter - 1) + len(s)]$卷积核为[dim, m]维度，卷积公式如下:$c_i=S \\cdot f=S^T_{[i:i+m-1]} \\cdot f=\\sum_{k=i}^{i+m-1} s_kf_k$注意是S的转置矩阵与f的相乘, 最后得到的是$[2 \\cdot filter + |s|]$的宽度数组，即为卷积结果。增加filter_width是为了让S的每个word享受到相同的卷积效果。 Activation (tanh)使用nn_layers.NonLinearityLayer作为主类，初始化以及output函数:1234NonLinearityLayer(b_size=filter_shape[0], activation=activation)def output_func(self, input): return self.activation(input + self.b.dimshuffle(&apos;x&apos;, 0, &apos;x&apos;, &apos;x&apos;)) dimshuffle是将b拓展到4维，正好与卷积的output匹配相加。这里有一个问题，卷积出的第四维度是ndim，和公式算的$c_i$没有该维度，该维度上直接相加了。 Pooling123# 在第二维度上，即image height，也就是sentencedef max_pooling(input): return T.max(input, axis=2) PairwiseNoFeatsLayer这个是将q和a经过上面一系列的层产生的结果(最后是经过Pooling处理得到的[batch_size, n])做乘积，求q和a的相似性sim，最后将q,a,sim拼接。12345678# 注意这里的参数W矩阵是(n * n)的def output_func(self, input): # P(Y|X) = softmax(W.X + b) q, a = input[0], input[1] # dot = T.batched_dot(q, T.batched_dot(a, self.W)) dot = T.batched_dot(q, T.dot(a, self.W.T)) out = T.concatenate([dot.dimshuffle(0, &apos;x&apos;), q, a], axis=1) return out out的长度为q_logistic_n_in + a_logistic_n_in + 1 LinearLayer线性变换，这里的输入是长度为q_logistic_n_in + a_logistic_n_in + 1的特征数组，需要经过线性变化，LinearLayer123# W是(q_logistic_n_in + a_logistic_n_in + 1) * (q_logistic_n_in + a_logistic_n_in + 1)def output_func(self, input): return self.activation(T.dot(input, self.W) + self.b) LogisticRegression最后一步，上面的输入仍然是q_logistic_n_in + a_logistic_n_in + 1, 使用softmax输出求解，12345def output_func(self, input): # P(Y|X) = softmax(W.X + b) self.p_y_given_x = T.nnet.softmax(self._dot(input, self.W) + self.b) self.y_pred = T.argmax(self.p_y_given_x, axis=1) return self.y_pred 注意T.argmax是获取下标，所以这里的输出是直接分类(本问题中是0,1), p_y_given_x是给的具体的概率值。 训练网络前面的整个网络已经布局完毕，是计算loss的时候了，cost=negative_log_likelihood(y)=-T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y]), 使用adadelta训练参数，这个cost如何理解呢？找到了解释，我自己是没有跑通，囧。1234567891011# y.shape[0] is (symbolically) the number of rows in y, i.e.,# number of examples (call it n) in the minibatch# T.arange(y.shape[0]) is a symbolic vector which will contain# [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of# Log-Probabilities (call it LP) with one row per example and# one column per class LP[T.arange(y.shape[0]),y] is a vector# v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,# LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is# the mean (across minibatch examples) of the elements in v,# i.e., the mean log-likelihood across the minibatch.return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y]) 如何更新参数呢？1updates = sgd_trainer.get_adadelta_updates(cost, params, rho=0.95, eps=1e-6, max_norm=max_norm, word_vec_name=&apos;W_emb&apos;) 作者实现的adadelta算法如下所示, 加上了我自己的理解和注释，我感觉这块的实现有些问题，因为adadelta是前t次的迭代，而这里的和显然是从开始到结束，有点像adagrad。后面贴的链接是几种求参迭代的方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546def get_adadelta_updates(cost, params, rho=0.95, eps=1e-6, max_norm=9, word_vec_name=&apos;W_emb&apos;): print &quot;Generating adadelta updates&quot; updates = OrderedDict(&#123;&#125;) exp_sqr_grads = OrderedDict(&#123;&#125;) exp_sqr_ups = OrderedDict(&#123;&#125;) gparams = [] # 这里设置了三个变量, for every param： # exp_sqr_grads init zeros # exp_sqr_ups zeros # gparams [] list append gp(grad) for param in params: exp_sqr_grads[param] = build_shared_zeros(param.shape.eval(), name=&quot;exp_grad_%s&quot; % param.name) print &apos;cost &apos;, cost print &apos;param.shape.eval&apos;, param.shape.eval() gp = T.grad(cost, param) print &apos;gp&apos;, gp exp_sqr_ups[param] = build_shared_zeros(param.shape.eval(), name=&quot;exp_grad_%s&quot; % param.name) print &apos;exp_sqr_ups&apos;, exp_sqr_ups[param].shape gparams.append(gp) print param, gp #真正的计算, for every param # exp_sg 之前的 sum(sqr(gp) 1...t-1) for now param # exp_su 之前的 sum(step 1...t-1) for now param # up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp); 加上现在的gp平方 # updates[exp_sg] = up_exp_sg 换上 # step = -(T.sqrt(exp_su + eps) / T.sqrt(up_exp_sg + eps)) * gp,该次更新的param step, 注意这里分母的gp是1..t，而分子的sum(step)是 1...t-1 #updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step); 更新本次 #stepped_param = param + step;最后目标 for param, gp in zip(params, gparams): exp_sg = exp_sqr_grads[param] exp_su = exp_sqr_ups[param] up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp) updates[exp_sg] = up_exp_sg step = -(T.sqrt(exp_su + eps) / T.sqrt(up_exp_sg + eps)) * gp updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step) stepped_param = param + step # if (param.get_value(borrow=True).ndim == 2) and (param.name != word_vec_name): if max_norm and param.name != word_vec_name: col_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0)) desired_norms = T.clip(col_norms, 0, T.sqrt(max_norm)) scale = desired_norms / (1e-7 + col_norms) updates[param] = stepped_param * scale else: updates[param] = stepped_param return updates 参考资料:http://blog.csdn.net/luo123n/article/details/48239963http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentvariantshttp://www.cnblogs.com/neopenx/p/4768388.htmlhttp://deeplearning.net/tutorial/code/lstm.py","categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"},{"name":"ml","slug":"ml","permalink":"http://yoursite.com/tags/ml/"}]},{"title":"Xgboost源码阅读","slug":"Xgboost","date":"2017-02-15T06:25:57.000Z","updated":"2017-06-14T11:51:06.000Z","comments":true,"path":"2017/02/15/Xgboost/","link":"","permalink":"http://yoursite.com/2017/02/15/Xgboost/","excerpt":"","text":"前言xgboost是有监督学习的利器，相较于与决策树、随机森林、gbdt等都有着明显优势，也许是各个算法有自己适用的领域吧。xgboost厉害之处在于同时控制了残差和复杂度，在迭代优化过程中，会同时考量这两个因素，本文会按照我自己的学习时的思路来介绍该算法，包括陈天奇(天才少年)的C++源码解读。 带着问题走由于之前也是使用了挺多的算法，包括dtree, random forest, 还有各种pair wise、list wise算法，但是听到同事的介绍还是产生了兴趣，我本人介入机器学习领域时间较短，开始在听完xgboost的介绍时，就迫切的想了解两个东西: xgboost如何并行化？xgboost是如何控制复杂度的?xgboost如何选择最优分裂? 这三个问题是我继续了解的动力，另外后面的阅读和学习中，又知道了关于xgboost的最优化split、最优化Gain等详细知识，另外还有各种objective 灵活设置，非常方便。有些材料仅供参考: xgboost官网，github某乎较好的帖子blog地址陈天奇slides陈天奇paper 原理介绍?基本的概念如tree, mart, residual等就不介绍了，直接进入主题，xgboost的目标函数: $Obj(\\theta) = L(\\theta) + \\Omega(\\theta)$$\\theta$是我们要求的最优解，可以对应xgboost上各个tree上最有split和最有leaf value，$L$是残差，即模型预测值和实际训练样本的差值，当然是越小越好，$\\Omega$是模型复杂度，这个也很重要，防止过拟合以及严重的抖动，一般模型需要做bias和varience的校验，一个对应$L$，一个对应$\\Omega$。xgboost是mart，预测值是所有的tree的加权相加:$\\hat{y_i}=\\sum_{i=1}^{K}f_k(x_i),f_k \\in{\\mathcal{F}}$以上是mart建立之后如何预测，tree建立在分裂时，会有目标，Objective细化为$Obj=\\sum_{i=1}^{n} l(y_i, \\hat{y_i})+\\sum_{k=1}^{K} \\Omega(f_i)=\\sum_{i=1}^{n}l(y_i, \\hat y_i^{t-1} + f_t(x_i)) + \\Omega(f_t) + constant $$f_t$是第t轮迭代，即第t颗树，由于t轮之前的参数已经确定，所以最后归结为constant，目标也是寻找最优的$f_t$，让Obj最小。使用二阶泰勒展开的方式$f(x+\\Delta{x}) \\approx f(x) + f’(x)\\Delta{x} + \\frac{1}{2}f’’(x)\\Delta{x^2}$做一下转化: $l(y_i, \\hat y_i^{t-1} + f_t(x_i)) =&gt; f(x+\\Delta{x}) $$g_i=\\delta_{\\hat y_i^{t-1}}{l(y_i, \\hat{y}^{(t-1)})}$$h_i=\\delta_{\\hat y_i^{t-1}}^2 {l(y_i, \\hat{y}^{(t-1)})}$ 则Obj在t轮迭代的目标为$Obj=\\sum_{i=1}^{n}l(y_i, \\hat y_i^{t-1} + f_t(x_i)) + \\Omega(f_t) + constant=\\sum_{i=1}^{n}[l(y_i, \\hat y_i^{t-1}) + g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)] + \\Omega(f_t) + constant$模型复杂度$\\Omega(f_t)$的定义：$\\Omega(f_t)=\\gamma{T} + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j{^2}$$T$是叶子节点的个数，$w_j$是叶子节点对叶的leaf_score,进一步化简$Obj(f_t)$:$Obj=\\sum_{i=1}^{n}[g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)] + \\Omega(f_t)=\\sum_{i=1}^{n}[g_iw_{q(x_i)} + \\frac{1}{2}h_iw^2_{q(x_i)}] + \\gamma{T} + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j{^2}$$=\\sum_{j=1}^{T}[(\\sum_{i \\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i \\in I_j}h_i + \\lambda)w^2_j] + \\gamma T$ 这样终于看出GBDT计算收益的最终公式了：$(1) w_j = - \\frac{G_j}{H_j+\\lambda}$$(2) Obj =- \\frac{1}{2}\\sum_{j=1}^{T}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T$$(3) Gain = \\frac{1}{2}[\\frac{G^2_L}{H_L+\\lambda} + \\frac{G^2_R}{H_R+\\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}] - \\gamma$ $\\gamma$就是传说中的一阶正则项，$\\gamma$是后面提到的参数min_split_loss，剪枝使用，太小的收益不必分裂。树节点分裂的收益达到某个值后再分裂，而二阶正则项$\\lambda$主要是为了防止过拟合, 增强模型的稳定性。 如何并行化？xgboost相对于gbdt是有一个并行的优势，怎么并行的呢，下面会读数据并行化和树分裂时的并行化分两部分介绍 读数据并行开始的输入数据是libsvm格式，形如”1 1:1.2 2:2.1 3:3.5 …”一行一行的数据，训练集合可能会非常大，大家都知道xgboost会枚举所有的特征值，选用最佳分割，所以开始是需要按列把各个feature排序好，这一步在xgboost里是并行做的。。。xgboost使用DMatrix存储训练数据，每次训练前会调用InitColAccess()将行训练数据支持按列取的数据，这里是通过MakeOneBatch多线程来实现的，这么多列属性的排序被n个线程平分，最后将数据转化成 key,value的方式，key是列id，value是rowid, val的pair，rowid是该列所对应的行编号，用于拿出整行训练数据的信息，val是该行该列下的具体数字，这个信息最后会存入DMatrix的SparsePage中，可知道最后是存了feature_num个实际train_set，用于训练阶段方便的取数据，不用做重复的排序工作了。 分裂选择并行化tree进行分裂的时候会考量每个feature的每个value，如果串行计算，肯定会比较慢，xgboost使用了并行化，多个线程综合考虑每个列的情况，主要在FindSplit()里做的● this-&gt;UpdateSolution(iter-&gt;Value(), gpair, *p_fmat); 主函数，枚举每一个feature，的每一个value，尝试收益● this-&gt;SyncBestSolution(qexpand); 多线程merger，当前node的最好的split● sync solution; 同步成果，给tree建node，左右儿子等每个线程会枚举自己的feature的各个split value，将最佳分裂保存到当前线程的结构中，最后所有的线程会根据各自的best split做merge，从而找到当前tree的最佳分裂，这里也用到了并行化，共享了读数据阶段产生的SparsePage 这些并行化和共享让xgboost的训练速度非常快。以上基本上解决了我开始的三个疑问，下面说一些xgboost的细节。 细节1: tree如何维护当前的数据集？这个是通过postion这个vector来实现的，vector是训练机的大小，每个值代表当前训练样本属于的树节点编号，开始都是0，即开始所有的样本都是在根节点上。expand这个vector保存当前叶子节点，即即将开始下一轮分裂的叶子节点编号，当然开始也是初始化为0，postion里的所有值都是取自于expand 细节2: 模型复杂度主要说下$\\gamma,\\alpha, \\lambda$这三个，这三个控制这模型复杂度。 $\\gamma$是min_split_loss，控制树的分裂深度，收益必须大于该值才分裂。 $\\alpha$是一阶正则项，也是控制复杂度的，不过比较弱，一般训练设置为0，控制这Grad(一阶梯度)不至于过大或者过小 123456if (p.reg_alpha == 0.0f) &#123; return Sqr(sum_grad) / (sum_hess + p.reg_lambda);&#125; else &#123; return Sqr(ThresholdL1(sum_grad, p.reg_alpha)) / (sum_hess + p.reg_lambda);&#125; $\\lambda$是二阶正则项，具体使用上面的推到也已经明了了 Lambda Object非常重要的应用，相对于gbdt，或许这个才算是最重要的区别，即可以自定义目标函数(loss)，传统的Objective loss即预测值和标签值的差异，在实际工程中的应用显然是不够的，很多指标有实际的公式，如果表示成残差，梯度，这个活就交给lambda来做了，如下要素:● 组的概念● 选择两个组内两个标签不同的一对● $P_{ij} = \\frac{1}{1+e^{-\\sigma(s_i-s_j)}}$● $C = -\\bar P_{ij} log P_{ij} - (1 - \\bar P_{ij})log(1 - P_{ij})$● $C=log(1+e^{-\\sigma(s_i-s_j)})$● $\\delta{w_k} = -\\eta\\sum_i\\lambda_{i}\\frac{\\partial s_i}{\\partial w_k}$● $\\lambda_i = \\sum_{j:{i, j}\\in I} \\lambda_{ij} - \\sum_{j:{j, i}\\in I} \\lambda_{ij}$● $\\lambda_{ij} = \\frac{\\partial C(s_i - s_j)}{\\partial{s_i}} = \\frac{-\\sigma}{1+e^{\\sigma(s_i-s_j)}} * \\Delta$ 这样，成功的讲工程上的知道公式产生的残差计算成功的引入到了梯度中去,$\\Delta$可以是NDCG以及MAP等等。如何从pointwise的思路转到pairwise listwise，$\\lambda$的应用是关键，其中的转化思路如上面的推理所示，具体代码在src/objective/rank_obj.cc中，定义了基本的类LambdaRankObj，在此基础上实现了PairwiseRankObj和LambdaRankObjNDCG以及LambdaRankObjMAP等。LambdaRankObj主要的函数就是12345678void GetGradient(const std::vector&lt;bst_float&gt;&amp; preds, const MetaInfo&amp; info, int iter, std::vector&lt;bst_gpair&gt;* out_gpair)preds: 当前预测值(fx)info: 样本信息iter: 当前迭代轮数(随机种子)out_gpair: 生成的一阶梯度、二阶梯度存放地 该函数的流程如下 遍历group 组内排序，按预测值放入lst, 按照标签值放入rec rec内按照不同的标签组成pairs, 采用uniform_int_distribution的方式组 调用GetLambdaWeight获取$\\Delta$, pairwise的什么也没干，默认是1 根据预测值和标签，算Sigmod值，然后乘$\\Delta$, 算出的g,h分别放入 out_gpair 的 i, j中，i &gt; j, 一个是加，一个是减1234gpair[pos.rindex].grad += g * w;gpair[pos.rindex].hess += 2.0f * w * h;gpair[neg.rindex].grad -= g * w;gpair[neg.rindex].hess += 2.0f * w * h; NDCG的GetLambdaWeight怎么算？这里就引入了计算公式。1234void GetLambdaWeight(const std::vector&lt;ListEntry&gt; &amp;sorted_list, std::vector&lt;LambdaPair&gt; *io_pairs) overridesorted_list:按照预测值排的序io_pairs:抽出的pairs 计算流程： 按照labels，算出最大的maxNDCG作为分母 算original ndcg以及交换之后的ndcg，做差值，处maxNDCG 问题1:如何做到好的结果按照NDCG更快排上去的?好的结果产生的$\\Delta$在NDCG计算时会更大，weight会更高，所以获得加权会更高 问题2:好的结果每次都会加权，最后预测值会很大？不会的, 如果好的记过得到的score很高了，即$s_i$很高，会拉开与其他结果($s_j$)的差距，所以系数$\\frac{-\\sigma}{1+e^{\\sigma(s_i-s_j)}}$会很小，这个系数表示目前模型的好坏，如果很好了即使$\\Delta$很大也是没有多少梯度的 应用参数首先推荐官方的API，这个没有更权威的了，如果使用，一定要仔细阅读，我是在python中使用，所以会看python的API)。首先说一下命令行版本也就是github拉下来之后直接编译出的二进制，直接使用”./xgboost config.ini”的方式即可，注意配置的格式，其实参考”src/cli_main.cc”也可以大概看出需要配置那些参数，我贴一下自己的训练配置和预测配置:123456789101112131415161718● train.conftrain_path=/Users/wang/company/ltr/ask.trainmodel_out=/Users/wang/company/ltr/module.out.1objective=rank:ndcgnum_round=200eval_metric=ndcg@3-task=traineval_train=truemax_depth=4min_split_loss=0.2reg_lambda=2.0● test.conftest_path=feature.askname_pred=pred.txtmodel_in=module.outobjective=rank:ndcgtask=pred 源码中是使用dmlc-core来做的参数处理，十分方便，我是做搜索排序相关，所以目标使用ndcg, reg_lambda对应上面所说的$\\lambda$，min_split_loss是树分裂所达到的最小收益值，即上面公式(3)中的Gain，一阶正则项$\\alpha＝0$。不过还是推荐使用python版本，python版本在训练时，可以看到每轮之后test集的准确度(ndcg值)，可以直观的看到效果，主要是param的设置，如下:12345678# paramparam = &#123;&apos;eval_metric&apos;: &apos;ndcg@3-&apos;, &apos;base_score&apos;: 0.0, &apos;max_leaves&apos;: 10, &apos;alpha&apos;: 0.0, &apos;tree_method&apos;: &apos;exact&apos;, &apos;bagging&apos;: 3, &apos;silent&apos;: 1, &apos;grow_policy&apos;: &apos;depthwise&apos;, &apos;subsample&apos;: 1.0, &apos;eta&apos;: 0.2, &apos;max_bin&apos;: 256, &apos;objective&apos;: &apos;rank:ndcg&apos;, &apos;max_depth&apos;: 4, &apos;gamma&apos;: 0.2, &apos;lambda&apos;: 2.0&#125;#设置测试集合的trainnum_round = 200dtrain.set_group(numpy.loadtxt(&apos;../../company/ltr/ask.train.group&apos;, dtype=numpy.int32))dtest.set_group(numpy.loadtxt(&apos;../../company/ltr/ask.test.group&apos;, dtype=numpy.int32))bst = xgb.train(param, dtrain, num_round, [(dtrain, &apos;train&apos;), (dtest, &apos;test&apos;)]) 最后贴一张训练截图","categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"},{"name":"ml","slug":"ml","permalink":"http://yoursite.com/tags/ml/"}]}],"categories":[],"tags":[{"name":"it","slug":"it","permalink":"http://yoursite.com/tags/it/"},{"name":"ml","slug":"ml","permalink":"http://yoursite.com/tags/ml/"},{"name":"life","slug":"life","permalink":"http://yoursite.com/tags/life/"},{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]}