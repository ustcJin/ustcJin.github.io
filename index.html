<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> Hexo </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Hexo</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/30/rotate-array-search/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="ustcJin">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/heart_scale.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/30/rotate-array-search/" itemprop="url">
                  rotate array search
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-30T10:07:18+08:00">
                2017-04-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>leetcode上遇到了一个问题，就是如果把一个sorted数组旋转一下，如[0,1,2,3,4,5,6,7] =&gt; [4,5,6,7,0,1,2,3]，这样二分查找还将如何进行？,当然是用O(log(n))的方法。</p>
<h1 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h1><p>应该是如下的场景，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">int search(vector&lt;int&gt; nums, int target) &#123;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="不重复的情况"><a href="#不重复的情况" class="headerlink" title="不重复的情况"></a>不重复的情况</h1><p>这里会有一个问题场景的问题，即数组中是否允许存在重复的元素? 如果不允许，则找到旋转的那个点，如例子中是在value=0处，即idx＝4，找到了这个点的方法有些多，可以使用寻找最小值的方法，也可以直接二分查找，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">int findMinIdx(vector&lt;int&gt; nums) &#123;</div><div class="line">	int lo = 0, hi = nums.size() - 1;</div><div class="line">	while(lo &lt; hi) &#123;</div><div class="line">		int mid = lo + (hi - lo) / 2;</div><div class="line">		if(nums[mid] &gt; nums[hi]) &#123;</div><div class="line">			lo = mid + 1;</div><div class="line">		&#125;</div><div class="line">		else &#123;</div><div class="line">			hi = mid - 1;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	// lo==hi is the index of the smallest value and also the number of places rotated.</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>也可以直接二分的算法查找，只不过原来的方法要稍微变动下，这种方法比较高效，不够思路有些不太清晰。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">int search(vector&lt;int&gt; nums, int target) &#123;</div><div class="line">	int start = 0, end = nums.end() - 1;</div><div class="line">	while(start &lt; end) &#123;</div><div class="line">		int mid = start + (end - start) / 2;</div><div class="line">		if(nums[mid] == target) &#123;</div><div class="line">			return mid;</div><div class="line">		&#125;</div><div class="line">		else if(nums[mid] &gt; nums[start]) &#123;</div><div class="line">			if(target &lt; nums[mid] &amp;&amp; target &gt;= nums[start]) &#123;</div><div class="line">				end = mid - 1;</div><div class="line">			&#125;</div><div class="line">			else &#123;</div><div class="line">				start = mid + 1;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		else if(nums[mid] &lt; nums[start])&#123;</div><div class="line">			if(target &gt;= nums[start] || target &lt; nums[mid]) &#123;</div><div class="line">				end = mid - 1;</div><div class="line">			&#125;</div><div class="line">			else &#123;</div><div class="line">				start = mid + 1;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		else &#123;</div><div class="line">			start++;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	return -1;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="重复场景如何？"><a href="#重复场景如何？" class="headerlink" title="重复场景如何？"></a>重复场景如何？</h1><p>这个没有争议，关键点在于如果数组中允许存在相同的元素，这个算法该怎么写，可以肯定的是上面的算法都将失效，比如[4,4,4,4,0,1,2,4]找不到最小的地方的，还有[4,6,4,4,4,4,4,4]如何找到rotated place是2，很难，经过验证，无法在log(n)的时间内找到翻转的位置，不过可以找到最小的值是多少，至于在哪个位置就不确定了，即最多找出一个最小值的idx。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">int findMinValue(vector&lt;int&gt; nums, int start, int end) &#123;</div><div class="line">	if(start &gt; end) &#123;</div><div class="line">		return -1;</div><div class="line">	&#125;</div><div class="line">	int mid = start + (end - start) / 2;</div><div class="line">	while(start &lt; end) &#123;</div><div class="line">		if(nums[mid] &gt; nums[start]) &#123; </div><div class="line">			int pos = findMinValue(nums, mid + 1, end);</div><div class="line">			if(pos == -1) &#123;</div><div class="line">				return start;</div><div class="line">			&#125;</div><div class="line">			return nums[start] &lt; nums[pos] ? start : pos;</div><div class="line">		&#125;</div><div class="line">		else if(nums[mid] &lt; nums[start]) &#123;</div><div class="line">			int pos = findMinValue(nums, start, mid - 1);</div><div class="line">			if(pos == -1) &#123;</div><div class="line">				return mid;</div><div class="line">			&#125;</div><div class="line">			return nums[pos] &lt; nums[mid] ? pos : mid;</div><div class="line">		&#125;</div><div class="line">		else &#123; //相等的情况较为复杂, 两边都要考虑</div><div class="line">			int pos = mid;</div><div class="line">			int pos1 = findMinValue(nums, start, mid - 1);</div><div class="line">			if(pos1 != -1) &#123;</div><div class="line">				pos = nums[pos] &lt; nums[pos1] ? pos : pos1;</div><div class="line">			&#125;</div><div class="line">			int pos2 = findMinValue(nums, mid + 1, end);</div><div class="line">			if(pos2 != -1) &#123;</div><div class="line">				pos = nums[pos] &lt; nums[pos2] ? pos :pos2;</div><div class="line">			&#125;</div><div class="line">			return pos;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	return -1;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/20/base-dt/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="ustcJin">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/heart_scale.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/20/base-dt/" itemprop="url">
                  基本的决策树算法总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-20T19:34:43+08:00">
                2017-03-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇文章仅仅是简单的回顾一下决策树开始的一些经典算法，虽然较旧了，用的人少了，但是一些思想和算法，值得了解。下面主要介绍的算法有<code>ID3</code>、<code>C4.5</code>、<code>CART</code>、<code>SLIQ</code>、<code>SPRINT</code>，其中<code>CART</code>算法一笔带过吧，有些需要深究的部分，我也没有太理清，就把我现在理解的东西权当是记录一下把。</p>
<h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><p>ID3(Iterative Dichotomiser 3), 迭代的二分法分离模型version 3，是根据信息增益来做的决策，所以如何定义信息增益是个主要的问题？首先来看一下信息熵的定义:<br>$H(S) = -\sum_{i=1}^{m} P(u_i)logP(u_i)$<br>其中$S$是样例的集合，$P(u_i)=\frac{|u_i|}{|S|}$，一般认为越有序的数据熵比较低，越乱序的则熵比较大，显然我们分类的目的就是有序，减少熵。最后的信息增益为:<br>$Gain(S,A)=Entropy(S) - \sum_{v \in Value(A)}\frac{|S_v|}{|S|}Entropy(S_v)$<br>遍历所有属性即可，这个算法优点就是简单，奥卡姆剃刀原理，缺点也较明显，抗噪音不行，大数据，无剪枝。</p>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5是对ID3的改进，因为ID3分裂出的树不是二叉树，而是有多少属性就有多少分叉，所以如果分叉过多，容易导致各个分支上的信息熵为0，但是这种明显不是我们想要的，所以这里提出了信息增益率的概念，公式为:<br>$GainRatio(A) = \frac{Gain(A)}{Entropy(A)}$<br>分母是训练集关于A的分布情况，显然，越多越大，最后的增益率就会下降。</p>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>CART和C4.5类似，不过是使用gini算法计算收益，gini值越小，效果越好。接下来会讲一下gini算法的实现。</p>
<h2 id="gini"><a href="#gini" class="headerlink" title="gini"></a>gini</h2><p>基尼指数原来是是衡量贫富悬殊程度的标量。它的定义如下：我们首先收集社会上每一个人的总财富额，把它从少至大排序，计算它的累积函数（cumulative function），然后便可绘出图中的洛仑兹曲线（Lorenz curve）。图中横轴是人口比例的累积分布，竖轴是财富比例的累积分佈。<br><img src="/images/gini.jpg" alt="gini"><br>$A$和$B$是途中两个面积，基尼系数为$\frac{A}{A+B}$<br>越不均匀，比如钱都集中在少数人手里，则$B$趋向于0，gini系数很大，python简单的实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">def gini_coef(wealths):</div><div class="line">	cum_wealths = np.cumsum(sorted(np.append(wealths, 0)))</div><div class="line">	sum_wealths = cum_wealths[-1]</div><div class="line">	xarray = np.array(range(0, len(cum_wealths))) / np.float(len(cum_wealths)-1)</div><div class="line">	yarray = cum_wealths / sum_wealths</div><div class="line">	B = np.trapz(yarray, x=xarray)</div><div class="line">	A = 0.5 - B</div><div class="line">	return A / (A+B)</div></pre></td></tr></table></figure></p>
<p>应用到具体分类的gini定义如下:<br>$gini(S)=1-\sum_{j=1}^{n}p_j^2$<br>$p_j$为类j出现的概率，如果集合分成$S_1$和$S_2$部分，则分割后的gini为<br>$gini_{split}(S)=\frac{n_1}{n}gini(S_1)+\frac{n_2}{n}gini(S_2)$<br>则$GAIN_{gini} = gini(S) - gini_{split}(S)$，越大收益越高。</p>
<h2 id="如何收益大？"><a href="#如何收益大？" class="headerlink" title="如何收益大？"></a>如何收益大？</h2><p>说到gini系数，其实有点相悖？gini系数中如果财产都在一个人手里，那岂不是显得很”纯”? 其实这种情况的确是比较符合理想分类的情形，因为99%的都是穷人，1%的才是富人，不过这样想就错了，因为负例太负了，负例产生的波动太大了，因为太大了，造成了整体的均值不回太低，所以其它99%的产生的mse也是比较大的，其实可以这么理解，一共100个样本，和就是<code>1</code>，如果均匀分布即大家都是同类人，这样才是分类的最佳情况。</p>
<h2 id="SLIQ"><a href="#SLIQ" class="headerlink" title="SLIQ"></a>SLIQ</h2><p>super-vised learning in quest, 这个名字的确挺蛋疼的，应该叫<code>一种可伸缩的分类器</code>，这个算法有什么特点呢？如下总结<br>| BFS<br>| 直方图<br>| 属性表<br>| 类表<br>每个树节点都拥有了直方图(local)，属性表(global), 类表(global), 这样也才能使用BFS，特别每个树节点都拥有自己的直方图，可以统一扫一下属性表，然后整体更新树节点上的直方图，直方图如下表所示，假设只有两个分类<code>C1,C2</code>：<br>参数|描述</p>
<table>
<thead>
<tr>
<th>stats1</th>
<th>C1</th>
<th>C2</th>
</tr>
</thead>
<tbody>
<tr>
<td>left</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>right</td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>这个是最初的统计，其实实际在遍历中，可以用最大收益代替，并附上最大收益对应的<code>attribute</code>以及<code>split\_value</code>, 当然一般的算收益仍然是使用gini算法。</p>
<table>
<thead>
<tr>
<th>stats2</th>
<th>Gain</th>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>stats</td>
<td>2.5</td>
<td>a1</td>
<td>6</td>
</tr>
</tbody>
</table>
<p>每次扫描完毕，这当前所有的叶子节点(active nodes)上的直方图都会更新一次，决定了下一次的分裂, 如上面节点中的最佳分裂的属性是<code>a1</code>, $split_value=6$。优点很直观啊，全局一次排序。</p>
<h2 id="SPRINT"><a href="#SPRINT" class="headerlink" title="SPRINT"></a>SPRINT</h2><p>scalable parallelizable induction(归纳) of decision trees, sprint算法，强调了并行，是在SLIQ的基础上更进一步, 将类图合并到每个属性表中，单独存入节点中，属性表随着节点的分裂也被分割，所以就不存在BFS了，而是深度优先遍历的方式，每个节点拥有自己单独的属性表(每个属性都需要一张(sorted))，可以单独分裂，这个可以加快并行，当然存储最佳分类的还是要使用直方图，这个算法的缺点就是每次分裂，需要设计到非分割属性表的元素分裂，需要通过rowid(hash?)对应上，分割相对复杂。</p>
<h2 id="直方图如何对应xgboost"><a href="#直方图如何对应xgboost" class="headerlink" title="直方图如何对应xgboost?"></a>直方图如何对应xgboost?</h2><p>分布式加权直方图算法,这是xgboost里带的，为了解决数据无法一次载入内存或者在分布式情况下算法，</p>
<blockquote>
<p>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。<br><strong>直方图补充</strong><br>微软新出的LightGBM算法也是用到了直方图，的确，这个直方图和SLIQ以及SPRINT的直方图并无区别，直方图直接在gbdt中使用，的确起到了比较好的效果，不过相对于level-wise，每个节点都需要维护自己的数据，各个feature上sort的数据，比较麻烦，不过这样的好处就是deep-wise，脱离了深度的限制，另外，LightGBM中使用了bin value作为划分根据，即将feature value专为bin value，一般一个字节，即最多256种划分，这样的好处就是存储压力变小，另外不必扫描所有的feature value，只需扫描256次即可，还有一个就是数据分割时较快速，因为内存占用少，共享索引表，直接划分，还有做差的方式，即处理好一个分裂节点，另一个节点直接通过与父节点作差即可得到，速度又得到了提高。</p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/08/qa-cnn/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="ustcJin">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/heart_scale.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/08/qa-cnn/" itemprop="url">
                  CNN在问答领域识别中应用
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-08T18:51:28+08:00">
                2017-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>短文本question-answer匹配度识别问题，论文(Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks.pdf), github<a href="https://github.com/aseveryn/deep-qa" target="_blank" rel="external">实现</a>。github的作者基本上实现了文章里的算法，计算的精度也差不多，不过使用theano实现的，我准备使用tensorflow实现，并解决大规模预料训练太慢的问题。当然本文要开始从分析git的实现开始讲起，会穿插theano的一些语法，实现等，我自己也是从零学起的，且当给自己留个笔记吧。</p>
<h1 id="架构？"><a href="#架构？" class="headerlink" title="架构？"></a>架构？</h1><p>不能不提到的论文中经典的架构图。<br><img src="/images/deep_qa.jpg" alt=""><br>网络用文字梳理的结果如下:</p>
<ol>
<li>question answer并行处理成词向量</li>
<li>词向量与卷积核做运算得到n个len(q or a)维的向量</li>
<li>对2中得到的向量做Activation和Pooling处理，得到1维数组，大小为n</li>
<li>问题和答案抽出的1维数组分别与相似度矩阵M做乘积预算，得到相似度，即$A*M*Q=sim$</li>
<li>计算answer和question中的相同部分F，将A,sim,Q,F拼接成新的向量</li>
<li>将5中得到的向量做线性变化，即$\alpha(W*X+b)$</li>
<li>最后使用Softmax对6中得到的linear之后的向量做分类</li>
</ol>
<h2 id="使用theano的实现"><a href="#使用theano的实现" class="headerlink" title="使用theano的实现"></a>使用theano的实现</h2><p>作者的实现有一些改动，如第5步骤中计算得到的交叉部分F，这部分可以是交叉词的词向量，也可以是交叉词的idf，比重等，显然idf和比重更容易让人信服，不过这个值不好处理，作者并没有用，而是直接使用的交叉词的词向量直接拼到question和answer的词向量中，即将这部分直接放到开头做了。</p>
<h2 id="数据预处理-embedding"><a href="#数据预处理-embedding" class="headerlink" title="数据预处理(embedding)"></a>数据预处理(embedding)</h2><p>这部分主要是将question-answer对分词(英文就是空格直接分词), 然后根据word2vec，将问题和答案分解成word2vec的词向量，并收集全集词表，word2vec缺失的词使用默认值填充。难点在于训练集合的选取，这是有监督的学习，对于中文的海量语法来说，需要的训练集合的规模也是非常大的。</p>
<h2 id="Load"><a href="#Load" class="headerlink" title="Load"></a>Load</h2><p>主要load的数据有train, dev, test三类，train是训练参数使用，dev是验证参数，test是实测使用，其实dev和test可以归为一类，每类数据分为question, answer, overlap。</p>
<h2 id="词向量转化"><a href="#词向量转化" class="headerlink" title="词向量转化"></a>词向量转化</h2><p>LookupTableFast将question, answer, overlap转化为词向量。这里会讲pad部分(即卷积的宽度补上)，最呕输出的是[batch_size, 1, 2 * (q_size + q_filter_widths - 1), ndim], ndim是词向量的宽度(word2vec生成的词向量的size)，后面ndim的长度会拓展为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># sentence长度 + 交叉长度</div><div class="line">ndim = vocab_emb.shape[1] + vocab_emb_overlap.shape[1]</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">class LookupTableFastStatic(Layer):</div><div class="line">	def __init__(self, W=None, pad=None):</div><div class="line">		super(LookupTableFastStatic, self).__init__()</div><div class="line">		self.pad = pad</div><div class="line">		self.W = theano.shared(value=W, name=&apos;W_emb&apos;, borrow=True)</div><div class="line"></div><div class="line">		def output_func(self, input):</div><div class="line">			out = self.W[input.flatten()].reshape((input.shape[0], 1, input.shape[1], self.W.shape[1]))</div><div class="line">			if self.pad:</div><div class="line">				pad_matrix = T.zeros((out.shape[0], out.shape[1], self.pad, out.shape[3]))</div><div class="line">	  			out = T.concatenate([pad_matrix, out, pad_matrix], axis=2)</div><div class="line">	  		return out</div><div class="line"></div><div class="line">	  def __repr__(self):</div><div class="line">		  return &quot;&#123;&#125;: &#123;&#125;&quot;.format(self.__class__.__name__, self.W.shape.eval())</div></pre></td></tr></table></figure>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>定义Conv2dLayer来做卷积类使用，卷积的初始化如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def __init__(self, rng, filter_shape, input_shape=None, W=None):</div><div class="line"># @rng: 初始化种子</div><div class="line"># @filter_shape: 卷积核的样式，filter_shape = (nkernels, num_input_channels, filter_width, ndim), nkernels = 100, num_input_channels = 1, 可见与question长度无关。</div><div class="line"># @input_shape: 同上一层词向量层的输出</div><div class="line"># @self.W 也和filter_shape具有相同的shape，随机化一些数字</div></pre></td></tr></table></figure></p>
<p>卷积使用库函数，调用如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def output_func(self, input):</div><div class="line">	return conv.conv2d(input, self.W, border_mode=&apos;valid&apos;,</div><div class="line">			filter_shape=self.filter_shape,</div><div class="line">			image_shape=self.input_shape)</div></pre></td></tr></table></figure></p>
<p>看看<code>conv2d</code>函数的返回, 看到output的是$[batch_size, filter_size, sentence_size, ndim]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Parameters:	</div><div class="line">input (dmatrix of dtensor3) – Symbolic variable for images to be filtered.</div><div class="line">filters (dmatrix of dtensor3) – Symbolic variable containing filter values.</div><div class="line">border_mode (&#123;&apos;valid&apos;, &apos;full&apos;&#125;) – See scipy.signal.convolve2d.</div><div class="line">subsample – Factor by which to subsample output.</div><div class="line">image_shape (tuple of length 2 or 3) – ([number images,] image height, image width).</div><div class="line">filter_shape (tuple of length 2 or 3) – ([number filters,] filter height, filter width).</div><div class="line">kwargs – See theano.tensor.nnet.conv.conv2d.</div><div class="line">Returns:	</div><div class="line">Tensor of filtered images, with shape ([number images,] [number filters,] image height, image width)</div></pre></td></tr></table></figure></p>
<p><strong>何为卷积？如何卷积?</strong><br>卷积为分时加权，在词向量上表现为分段计算作为权重，粒度更细了，input_shape细化为单个qa，即为如下格式(加上filter_width)。引用某乎上对卷积的解释:</p>
<blockquote>
<p>cnn的核心在于卷积核，其实关于卷积核还有另一个名字叫做滤波器，从信号处理的角度而言，滤波器是对信号做频率筛选，这里主要是空间-频率的转换，cnn的训练就是找到最好的滤波器使得滤波后的信号更容易分类，还可以从模版匹配的角度看卷积，每个卷积核都可以看成一个特征模版，训练就是为了找到最适合分类的特征模板。</p>
</blockquote>
<p>卷积核最后是作为最终的feature，每个卷积核作为一个特征，与question、answer等都无关了，所以卷积核的特征采集是非常重要的。</p>
<p>$$<br>S =<br>\begin{vmatrix}<br>|    &amp; |    &amp;    …    &amp; | &amp; | \\<br>filter &amp; w_1 &amp; … &amp; w_{|s|} &amp; filter \\<br>|    &amp; |    &amp;    …    &amp; | &amp; | \\<br>\end{vmatrix}<br>$$<br>$S.shape = [ndim, 2 \cdot (filter - 1) + len(s)]$<br>卷积核为[dim, m]维度，卷积公式如下:<br>$c_i=S \cdot f=S^T_{[i:i+m-1]} \cdot f=\sum_{k=i}^{i+m-1} s_kf_k$<br>注意是S的转置矩阵与f的相乘, 最后得到的是$[2 \cdot filter + |s|]$的宽度数组，即为卷积结果。增加filter_width是为了让S的每个word享受到相同的卷积效果。</p>
<h2 id="Activation-tanh"><a href="#Activation-tanh" class="headerlink" title="Activation (tanh)"></a>Activation (tanh)</h2><p>使用nn_layers.NonLinearityLayer作为主类，初始化以及output函数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">NonLinearityLayer(b_size=filter_shape[0], activation=activation)</div><div class="line"></div><div class="line">def output_func(self, input):</div><div class="line">	return self.activation(input + self.b.dimshuffle(&apos;x&apos;, 0, &apos;x&apos;, &apos;x&apos;))</div></pre></td></tr></table></figure></p>
<p>dimshuffle是将b拓展到4维，正好与卷积的output匹配相加。这里有一个问题，卷积出的第四维度是ndim，和公式算的$c_i$没有该维度，该维度上直接相加了。</p>
<h1 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 在第二维度上，即image height，也就是sentence</div><div class="line">def max_pooling(input):</div><div class="line">  return T.max(input, axis=2)</div></pre></td></tr></table></figure>
<h1 id="PairwiseNoFeatsLayer"><a href="#PairwiseNoFeatsLayer" class="headerlink" title="PairwiseNoFeatsLayer"></a>PairwiseNoFeatsLayer</h1><p>这个是将q和a经过上面一系列的层产生的结果(最后是经过Pooling处理得到的[batch_size, n])做乘积，求q和a的相似性sim，最后将q,a,sim拼接。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 注意这里的参数W矩阵是(n * n)的</div><div class="line">def output_func(self, input):</div><div class="line">	# P(Y|X) = softmax(W.X + b)</div><div class="line">	q, a = input[0], input[1]</div><div class="line">	# dot = T.batched_dot(q, T.batched_dot(a, self.W))</div><div class="line">	dot = T.batched_dot(q, T.dot(a, self.W.T))</div><div class="line">	out = T.concatenate([dot.dimshuffle(0, &apos;x&apos;), q, a], axis=1)</div><div class="line">	return out</div></pre></td></tr></table></figure></p>
<p>out的长度为<code>q_logistic_n_in + a_logistic_n_in + 1</code></p>
<h1 id="LinearLayer"><a href="#LinearLayer" class="headerlink" title="LinearLayer"></a>LinearLayer</h1><p>线性变换，这里的输入是长度为<code>q_logistic_n_in + a_logistic_n_in + 1</code>的特征数组，需要经过线性变化，LinearLayer<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># W是(q_logistic_n_in + a_logistic_n_in + 1) * (q_logistic_n_in + a_logistic_n_in + 1)</div><div class="line">def output_func(self, input):</div><div class="line">    return self.activation(T.dot(input, self.W) + self.b)</div></pre></td></tr></table></figure></p>
<h1 id="LogisticRegression"><a href="#LogisticRegression" class="headerlink" title="LogisticRegression"></a>LogisticRegression</h1><p>最后一步，上面的输入仍然是<code>q_logistic_n_in  + a_logistic_n_in + 1</code>, 使用<code>softmax</code>输出求解，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def output_func(self, input):</div><div class="line">	# P(Y|X) = softmax(W.X + b)</div><div class="line">	self.p_y_given_x = T.nnet.softmax(self._dot(input, self.W) + self.b)</div><div class="line">	self.y_pred = T.argmax(self.p_y_given_x, axis=1)</div><div class="line">	return self.y_pred</div></pre></td></tr></table></figure></p>
<p>注意<code>T.argmax</code>是获取下标，所以这里的输出是直接分类(本问题中是0,1), p_y_given_x是给的具体的概率值。</p>
<h1 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h1><p>前面的整个网络已经布局完毕，$cost=negative_log_likelihood(y)=-T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])$, 使用adadelta训练参数，这个cost如何理解呢？找到了解释，我自己是没有跑通，囧。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># y.shape[0] is (symbolically) the number of rows in y, i.e.,</div><div class="line"># number of examples (call it n) in the minibatch</div><div class="line"># T.arange(y.shape[0]) is a symbolic vector which will contain</div><div class="line"># [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of</div><div class="line"># Log-Probabilities (call it LP) with one row per example and</div><div class="line"># one column per class LP[T.arange(y.shape[0]),y] is a vector</div><div class="line"># v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,</div><div class="line"># LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is</div><div class="line"># the mean (across minibatch examples) of the elements in v,</div><div class="line"># i.e., the mean log-likelihood across the minibatch.</div><div class="line">return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])</div></pre></td></tr></table></figure></p>
<p>如何更新参数呢？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">updates = sgd_trainer.get_adadelta_updates(cost, params, rho=0.95, eps=1e-6, max_norm=max_norm, word_vec_name=&apos;W_emb&apos;)</div></pre></td></tr></table></figure></p>
<p>作者实现的adadelta算法如下所示, 加上了我自己的理解和注释，我感觉这块的实现有些问题，因为adadelta是前t次的迭代，而这里的和显然是从开始到结束，有点像adagrad。后面贴的链接是几种求参迭代的方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">def get_adadelta_updates(cost, params, rho=0.95, eps=1e-6, max_norm=9, word_vec_name=&apos;W_emb&apos;):</div><div class="line">	print &quot;Generating adadelta updates&quot;</div><div class="line">	updates = OrderedDict(&#123;&#125;)</div><div class="line">	exp_sqr_grads = OrderedDict(&#123;&#125;)</div><div class="line">	exp_sqr_ups = OrderedDict(&#123;&#125;)</div><div class="line">	gparams = []</div><div class="line">	# 这里设置了三个变量, for every param：</div><div class="line">	# exp_sqr_grads init zeros</div><div class="line">	# exp_sqr_ups zeros</div><div class="line">	# gparams [] list append gp(grad)</div><div class="line">	for param in params:</div><div class="line">		exp_sqr_grads[param] = build_shared_zeros(param.shape.eval(), name=&quot;exp_grad_%s&quot; % param.name)</div><div class="line">		print &apos;cost &apos;, cost</div><div class="line">		print &apos;param.shape.eval&apos;, param.shape.eval()</div><div class="line">		gp = T.grad(cost, param)</div><div class="line">		print &apos;gp&apos;, gp</div><div class="line">		exp_sqr_ups[param] = build_shared_zeros(param.shape.eval(), name=&quot;exp_grad_%s&quot; % param.name)</div><div class="line">		print &apos;exp_sqr_ups&apos;, exp_sqr_ups[param].shape</div><div class="line">		gparams.append(gp)</div><div class="line">		print param, gp</div><div class="line">	#真正的计算, for every param</div><div class="line">	# exp_sg 之前的 sum(sqr(gp) 1...t-1) for now param</div><div class="line">	# exp_su 之前的 sum(step 1...t-1) for now param</div><div class="line">	# up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp); 加上现在的gp平方</div><div class="line">	# updates[exp_sg] = up_exp_sg 换上</div><div class="line">	# step =  -(T.sqrt(exp_su + eps) / T.sqrt(up_exp_sg + eps)) * gp,该次更新的param step,</div><div class="line">	  注意这里分母的gp是1..t，而分子的sum(step)是 1...t-1</div><div class="line">	#updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step); 更新本次</div><div class="line">	#stepped_param = param + step;最后目标</div><div class="line">	for param, gp in zip(params, gparams):</div><div class="line">		exp_sg = exp_sqr_grads[param]</div><div class="line">		exp_su = exp_sqr_ups[param]</div><div class="line">	 	up_exp_sg = rho * exp_sg + (1 - rho) * T.sqr(gp)</div><div class="line">	 	updates[exp_sg] = up_exp_sg</div><div class="line">		step =  -(T.sqrt(exp_su + eps) / T.sqrt(up_exp_sg + eps)) * gp</div><div class="line">	 	updates[exp_su] = rho * exp_su + (1 - rho) * T.sqr(step)</div><div class="line">	 	stepped_param = param + step</div><div class="line">		# if (param.get_value(borrow=True).ndim == 2) and (param.name != word_vec_name):</div><div class="line">	 	if max_norm and param.name != word_vec_name:</div><div class="line">			col_norms = T.sqrt(T.sum(T.sqr(stepped_param), axis=0))</div><div class="line">			desired_norms = T.clip(col_norms, 0, T.sqrt(max_norm))</div><div class="line">			scale = desired_norms / (1e-7 + col_norms)</div><div class="line">			updates[param] = stepped_param * scale</div><div class="line">		else:</div><div class="line">			updates[param] = stepped_param</div><div class="line">	return updates</div></pre></td></tr></table></figure></p>
<p>参考资料:<br><a href="http://blog.csdn.net/luo123n/article/details/48239963" target="_blank" rel="external">http://blog.csdn.net/luo123n/article/details/48239963</a>  \<br><a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentvariants" target="_blank" rel="external">http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentvariants</a> \<br><a href="http://www.cnblogs.com/neopenx/p/4768388.html" target="_blank" rel="external">http://www.cnblogs.com/neopenx/p/4768388.html</a> \<br><a href="http://deeplearning.net/tutorial/code/lstm.py" target="_blank" rel="external">http://deeplearning.net/tutorial/code/lstm.py</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/15/Xgboost/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="ustcJin">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/heart_scale.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/15/Xgboost/" itemprop="url">
                  Xgboost源码阅读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-15T14:25:57+08:00">
                2017-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>xgboost</code>是有监督学习的利器，相较于与决策树、随机森林、gbdt等都有着明显优势，也许是各个算法有自己适用的领域吧。<code>xgboost</code>厉害之处在于同时控制了残差和复杂度，在迭代优化过程中，会同时考量这两个因素，本文会按照我自己的学习时的思路来介绍该算法，包括陈天奇(天才少年)的C++源码解读。</p>
<h1 id="带着问题走"><a href="#带着问题走" class="headerlink" title="带着问题走"></a>带着问题走</h1><p>由于之前也是使用了挺多的算法，包括dtree, random forest, 还有各种pair wise、list wise算法，但是听到同事的介绍还是产生了兴趣，我本人介入机器学习领域时间较短，开始在听完<code>xgboost</code>的介绍时，就迫切的想了解两个东西: </p>
<blockquote>
<p>xgboost如何并行化？<br>xgboost是如何控制复杂度的?<br>xgboost如何选择最优分裂?</p>
</blockquote>
<p>这三个问题是我继续了解的动力，另外后面的阅读和学习中，又知道了关于<code>xgboost</code>的最优化split、最优化Gain等详细知识，另外还有各种objective 灵活设置，非常方便。有些材料仅供参考:</p>
<blockquote>
<p>xgboost<a href="xgboost.readthedocs.io/en/latest/">官网</a>，<a href="https://github.com/dmlc/xgboost" target="_blank" rel="external">github</a><br>某乎较好的<a href="https://www.zhihu.com/question/41354392" target="_blank" rel="external">帖子</a><br>blog<a href="http://blog.csdn.net/chedan541300521/article/details/54895880" target="_blank" rel="external">地址</a><br>陈天奇<a href="http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="external">slides</a><br>陈天奇<a href="https://arxiv.org/abs/1603.02754" target="_blank" rel="external">paper</a></p>
</blockquote>
<h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍?"></a>原理介绍?</h1><p>基本的概念如tree, mart, residual等就不介绍了，直接进入主题，<code>xgboost</code>的目标函数:<br>                             $Obj(\theta) = L(\theta) + \Omega(\theta)$<br>$\theta$是我们要求的最优解，可以对应<code>xgboost</code>上各个<code>tree</code>上最有<code>split</code>和最有<code>leaf value</code>，$L$是残差，即模型预测值和实际训练样本的差值，当然是越小越好，$\Omega$是模型复杂度，这个也很重要，防止过拟合以及严重的抖动，一般模型需要做<code>bias</code>和<code>varience</code>的校验，一个对应$L$，一个对应$\Omega$。<code>xgboost</code>是<code>mart</code>，预测值是所有的<code>tree</code>的加权相加:<br>$\hat{y_i}=\sum_{i=1}^{K}f_k(x_i),f_k \in{\mathcal{F}}$<br>以上是mart建立之后如何预测，tree建立在分裂时，会有目标，<code>Objective</code>细化为<br>$Obj=\sum_{i=1}^{n} l(y_i, \hat{y_i})+\sum_{k=1}^{K} \Omega(f_i)<br>=\sum_{i=1}^{n}l(y_i, \hat y_i^{t-1} + f_t(x_i)) + \Omega(f_t) + constant $<br>$f_t$是第t轮迭代，即第t颗树，由于t轮之前的参数已经确定，所以最后归结为<code>constant</code>，目标也是寻找最优的$f_t$，让<code>Obj</code>最小。<br>使用二阶泰勒展开的方式<br>$f(x+\Delta{x}) \approx f(x) + f’(x)\Delta{x} + \frac{1}{2}f’’(x)\Delta{x^2}$<br>做一下转化:</p>
<blockquote>
<p>$l(y_i, \hat y_i^{t-1} + f_t(x_i)) =&gt; f(x+\Delta{x}) $<br>$g_i=\delta_{\hat y_i^{t-1}}{l(y_i, \hat{y}^{(t-1)})}$<br>$h_i=\delta_{\hat y_i^{t-1}}^2 {l(y_i, \hat{y}^{(t-1)})}$</p>
</blockquote>
<p>则<code>Obj</code>在t轮迭代的目标为<br>$Obj=\sum_{i=1}^{n}l(y_i, \hat y_i^{t-1} + f_t(x_i)) + \Omega(f_t) + constant<br>=\sum_{i=1}^{n}[l(y_i, \hat y_i^{t-1}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t) + constant$<br>模型复杂度$\Omega(f_t)$的定义：<br>$\Omega(f_t)=\gamma{T} + \frac{1}{2}\lambda\sum_{j=1}^{T}w_j{^2}$<br>$T$是叶子节点的个数，$w_j$是叶子节点对叶的<code>leaf_score</code>,进一步化简$Obj(f_t)$:<br>$Obj=\sum_{i=1}^{n}[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)] + \Omega(f_t)<br>=\sum_{i=1}^{n}[g_iw_{q(x_i)} + \frac{1}{2}h_iw^2_{q(x_i)}] + \gamma{T} + \frac{1}{2}\lambda\sum_{j=1}^{T}w_j{^2}$<br>$=\sum_{j=1}^{T}[(\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda)w^2_j] + \gamma T$</p>
<p>这样终于看出GBDT计算收益的最终公式了：<br>$(1) w_j = - \frac{G_j}{H_j+\lambda}$<br>$(2) Obj =- \frac{1}{2}\sum_{j=1}^{T}\frac{G^2_j}{H_j + \lambda} + \gamma T$<br>$(3) Gain = \frac{1}{2}[\frac{G^2_L}{H_L+\lambda} + \frac{G^2_R}{H_R+\lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma$</p>
<p>$\gamma$就是<del>传说中的一阶正则项</del>，$\gamma$是后面提到的参数<code>min_split_loss</code>，剪枝使用，太小的收益不必分裂。树节点分裂的收益达到某个值后再分裂，而二阶正则项$\lambda$主要是为了防止过拟合, 增强模型的稳定性。</p>
<h1 id="如何并行化？"><a href="#如何并行化？" class="headerlink" title="如何并行化？"></a>如何并行化？</h1><p>xgboost相对于gbdt是有一个并行的优势，怎么并行的呢，下面会读数据并行化和树分裂时的并行化分两部分介绍</p>
<h2 id="读数据并行"><a href="#读数据并行" class="headerlink" title="读数据并行"></a>读数据并行</h2><p>开始的输入数据是libsvm格式，形如”1 1:1.2 2:2.1 3:3.5 …”一行一行的数据，训练集合可能会非常大，大家都知道xgboost会枚举所有的特征值，选用最佳分割，所以开始是需要按列把各个feature排序好，这一步在xgboost里是并行做的。。。xgboost使用DMatrix存储训练数据，每次训练前会调用InitColAccess()将行训练数据支持按列取的数据，这里是通过MakeOneBatch多线程来实现的，这么多列属性的排序被n个线程平分，最后将数据转化成 key,value的方式，key是列id，value是rowid, val的pair，rowid是该列所对应的行编号，用于拿出整行训练数据的信息，val是该行该列下的具体数字，这个信息最后会存入DMatrix的SparsePage中，可知道最后是存了feature_num个实际train_set，用于训练阶段方便的取数据，不用做重复的排序工作了。</p>
<h2 id="分裂选择并行化"><a href="#分裂选择并行化" class="headerlink" title="分裂选择并行化"></a>分裂选择并行化</h2><p>tree进行分裂的时候会考量每个feature的每个value，如果串行计算，肯定会比较慢，xgboost使用了并行化，多个线程综合考虑每个列的情况，主要在FindSplit()里做的<br>● this-&gt;UpdateSolution(iter-&gt;Value(), gpair, *p_fmat); 主函数，枚举每一个feature，的每一个value，尝试收益<br>● this-&gt;SyncBestSolution(qexpand); 多线程merger，当前node的最好的split<br>● sync solution; 同步成果，给tree建node，左右儿子等<br>每个线程会枚举自己的feature的各个split value，将最佳分裂保存到当前线程的结构中，最后所有的线程会根据各自的best split做merge，从而找到当前tree的最佳分裂，这里也用到了并行化，共享了读数据阶段产生的SparsePage</p>
<p>这些并行化和共享让xgboost的训练速度非常快。以上基本上解决了我开始的三个疑问，下面说一些xgboost的细节。</p>
<h2 id="细节1-tree如何维护当前的数据集？"><a href="#细节1-tree如何维护当前的数据集？" class="headerlink" title="细节1: tree如何维护当前的数据集？"></a>细节1: tree如何维护当前的数据集？</h2><p>这个是通过postion这个vector来实现的，vector是训练机的大小，每个值代表当前训练样本属于的树节点编号，开始都是0，即开始所有的样本都是在根节点上。expand这个vector保存当前叶子节点，即即将开始下一轮分裂的叶子节点编号，当然开始也是初始化为0，postion里的所有值都是取自于expand</p>
<h2 id="细节2-模型复杂度"><a href="#细节2-模型复杂度" class="headerlink" title="细节2: 模型复杂度"></a>细节2: 模型复杂度</h2><p>主要说下$\gamma,\alpha, \lambda$这三个，这三个控制这模型复杂度。</p>
<ul>
<li>$\gamma$是<code>min_split_loss</code>，控制树的分裂深度，收益必须大于该值才分裂。</li>
<li><p>$\alpha$是一阶正则项，也是控制复杂度的，不过比较弱，一般训练设置为0，控制这Grad(一阶梯度)不至于过大或者过小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">if (p.reg_alpha == 0.0f) &#123;</div><div class="line">	return Sqr(sum_grad) / (sum_hess + p.reg_lambda);</div><div class="line">&#125; else &#123;</div><div class="line">	return Sqr(ThresholdL1(sum_grad, p.reg_alpha)) /</div><div class="line">		(sum_hess + p.reg_lambda);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>$\lambda$是二阶正则项，具体使用上面的推到也已经明了了</p>
</li>
</ul>
<h1 id="Lambda-Object"><a href="#Lambda-Object" class="headerlink" title="Lambda Object"></a>Lambda Object</h1><p>非常重要的应用，相对于<code>gbdt</code>，或许这个才算是最重要的区别，即可以自定义目标函数(loss)，传统的<code>Objective loss</code>即预测值和标签值的差异，在实际工程中的应用显然是不够的，很多指标有实际的公式，如果表示成残差，梯度，这个活就交给<code>lambda</code>来做了，如下要素:<br>● 组的概念<br>● 选择两个组内两个标签不同的一对<br>● $P_{ij} = \frac{1}{1+e^{-\sigma(s_i-s_j)}}$<br>● $C = -\bar P_{ij} log P_{ij} - (1 - \bar P_{ij})log(1 - P_{ij})$<br>● $C=log(1+e^{-\sigma(s_i-s_j)})$<br>● $\delta{w_k} = -\eta\sum_i\lambda_{i}\frac{\partial s_i}{\partial w_k}$<br>● $\lambda_i = \sum_{j:{i, j}\in I} \lambda_{ij} - \sum_{j:{j, i}\in I} \lambda_{ij}$<br>● $\lambda_{ij} = \frac{\partial C(s_i - s_j)}{\partial{s_i}} = \frac{-\sigma}{1+e^{\sigma(s_i-s_j)}} * \Delta$</p>
<p>这样，成功的讲工程上的知道公式产生的残差计算成功的引入到了梯度中去,$\Delta$可以是<code>NDCG</code>以及<code>MAP</code>等等。如何从<code>pointwise</code>的思路转到<code>pairwise listwise</code>，$\lambda$的应用是关键，其中的转化思路如上面的推理所示，具体代码在<code>src/objective/rank_obj.cc</code>中，定义了基本的类<code>LambdaRankObj</code>，在此基础上实现了<code>PairwiseRankObj</code>和<code>LambdaRankObjNDCG</code>以及<code>LambdaRankObjMAP</code>等。<code>LambdaRankObj</code>主要的函数就是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">void GetGradient(const std::vector&lt;bst_float&gt;&amp; preds,</div><div class="line">	const MetaInfo&amp; info,</div><div class="line">	int iter,</div><div class="line">	std::vector&lt;bst_gpair&gt;* out_gpair)</div><div class="line">preds: 当前预测值(fx)</div><div class="line">info: 样本信息</div><div class="line">iter: 当前迭代轮数(随机种子)</div><div class="line">out_gpair: 生成的一阶梯度、二阶梯度存放地</div></pre></td></tr></table></figure></p>
<p>该函数的流程如下</p>
<ul>
<li>遍历group</li>
<li>组内排序，按预测值放入lst, 按照标签值放入rec</li>
<li>rec内按照不同的标签组成pairs, 采用uniform_int_distribution的方式组</li>
<li>调用GetLambdaWeight获取$\Delta$, pairwise的什么也没干，默认是1</li>
<li>根据预测值和标签，算Sigmod值，然后乘$\Delta$, 算出的g,h分别放入 out_gpair 的 i, j中，i &gt; j, 一个是加，一个是减<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">gpair[pos.rindex].grad += g * w;</div><div class="line">gpair[pos.rindex].hess += 2.0f * w * h;</div><div class="line">gpair[neg.rindex].grad -= g * w;</div><div class="line">gpair[neg.rindex].hess += 2.0f * w * h;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>NDCG的GetLambdaWeight怎么算？这里就引入了计算公式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">void GetLambdaWeight(const std::vector&lt;ListEntry&gt; &amp;sorted_list,</div><div class="line">                     std::vector&lt;LambdaPair&gt; *io_pairs) override</div><div class="line">sorted_list:按照预测值排的序</div><div class="line">io_pairs:抽出的pairs</div></pre></td></tr></table></figure></p>
<p>计算流程：</p>
<ul>
<li>按照labels，算出最大的maxNDCG作为分母</li>
<li>算original ndcg以及交换之后的ndcg，做差值，处maxNDCG</li>
</ul>
<h2 id="问题1-如何做到好的结果按照NDCG更快排上去的"><a href="#问题1-如何做到好的结果按照NDCG更快排上去的" class="headerlink" title="问题1:如何做到好的结果按照NDCG更快排上去的?"></a>问题1:如何做到好的结果按照NDCG更快排上去的?</h2><p>好的结果产生的$\Delta$在NDCG计算时会更大，weight会更高，所以获得加权会更高</p>
<h2 id="问题2-好的结果每次都会加权，最后预测值会很大？"><a href="#问题2-好的结果每次都会加权，最后预测值会很大？" class="headerlink" title="问题2:好的结果每次都会加权，最后预测值会很大？"></a>问题2:好的结果每次都会加权，最后预测值会很大？</h2><p>不会的, 如果好的记过得到的score很高了，即$s_i$很高，会拉开与其他结果($s_j$)的差距，所以系数$\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}$会很小，这个系数表示目前模型的好坏，如果很好了即使$\Delta$很大也是没有多少梯度的</p>
<h1 id="应用参数"><a href="#应用参数" class="headerlink" title="应用参数"></a>应用参数</h1><p>首先推荐官方的API，这个没有更权威的了，如果使用，一定要仔细阅读，我是在python中使用，所以会看<a href="http://xgboost.readthedocs.io/en/latest/python/python_api.html" target="_blank" rel="external">python的API</a>)。首先说一下命令行版本也就是github拉下来之后直接编译出的二进制，直接使用”./xgboost config.ini”的方式即可，注意配置的格式，其实参考”src/cli_main.cc”也可以大概看出需要配置那些参数，我贴一下自己的训练配置和预测配置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">● train.conf</div><div class="line">train_path=/Users/wang/company/ltr/ask.train</div><div class="line">model_out=/Users/wang/company/ltr/module.out.1</div><div class="line">objective=rank:ndcg</div><div class="line">num_round=200</div><div class="line">eval_metric=ndcg@3-</div><div class="line">task=train</div><div class="line">eval_train=true</div><div class="line">max_depth=4</div><div class="line">min_split_loss=0.2</div><div class="line">reg_lambda=2.0</div><div class="line"></div><div class="line">● test.conf</div><div class="line">test_path=feature.ask</div><div class="line">name_pred=pred.txt</div><div class="line">model_in=module.out</div><div class="line">objective=rank:ndcg</div><div class="line">task=pred</div></pre></td></tr></table></figure></p>
<p>源码中是使用<a href="https://github.com/dmlc/dmlc-core" target="_blank" rel="external">dmlc-core</a>来做的参数处理，十分方便，我是做搜索排序相关，所以目标使用<code>ndcg, reg_lambda</code>对应上面所说的$\lambda$，<code>min_split_loss</code>是树分裂所达到的最小收益值，即上面公式(3)中的Gain，一阶正则项$\alpha＝0$。<br>不过还是推荐使用python版本，python版本在训练时，可以看到每轮之后test集的准确度(ndcg值)，可以直观的看到效果，主要是param的设置，如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># param</div><div class="line">param = &#123;&apos;eval_metric&apos;: &apos;ndcg@3-&apos;, &apos;base_score&apos;: 0.0, &apos;max_leaves&apos;: 10, &apos;alpha&apos;: 0.0, &apos;tree_method&apos;: &apos;exact&apos;, &apos;bagging&apos;: 3, &apos;silent&apos;: 1, &apos;grow_policy&apos;: &apos;depthwise&apos;, &apos;subsample&apos;: 1.0, &apos;eta&apos;: 0.2, &apos;max_bin&apos;: 256, &apos;objective&apos;: &apos;rank:ndcg&apos;, &apos;max_depth&apos;: 4, &apos;gamma&apos;: 0.2, &apos;lambda&apos;: 2.0&#125;</div><div class="line"></div><div class="line">#设置测试集合的train</div><div class="line">num_round = 200</div><div class="line">dtrain.set_group(numpy.loadtxt(&apos;../../company/ltr/ask.train.group&apos;, dtype=numpy.int32))</div><div class="line">dtest.set_group(numpy.loadtxt(&apos;../../company/ltr/ask.test.group&apos;, dtype=numpy.int32))</div><div class="line">bst = xgb.train(param, dtrain, num_round, [(dtrain, &apos;train&apos;), (dtest, &apos;test&apos;)])</div></pre></td></tr></table></figure></p>
<p>最后贴一张训练截图<br><img src="/images/xgboost_ndcg.jpg" alt="ndcg"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  

          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/heart_scale.png"
               alt="ustcJin" />
          <p class="site-author-name" itemprop="name">ustcJin</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ustcJin</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


</body>
</html>
